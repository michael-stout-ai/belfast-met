## 6.1 Image Processing Basics

**Terms, Acronyms, and Definitions:**

* **Image Processing**: Involves manipulating digital images to improve their quality, extract meaningful information, or prepare them for further analysis.
* **Pixel**: A fundamental component of digital images, representing a small area of the image, each typically with an intensity value.
* **Grayscale Images**: Images where each pixel is represented by a single intensity value, typically ranging from 0 (black) to 255 (white).
* **RGB Images**: Images where colors are represented using three channels: red, green, and blue, with each pixel containing a combination of intensity values from these channels.
* **Alternative Color Spaces**: Beyond RGB, formats like HSV (Hue, Saturation, Value), YUV (luminance and chrominance), and CMYK (Cyan, Magenta, Yellow, Black), each offering specific advantages for different image processing tasks.
* **Smoothing Filters**: Blurring filters that reduce noise and smooth sharp transitions in images by averaging pixel values within a local neighborhood.
* **Gaussian Blur**: A type of smoothing filter that applies a weighted average based on a Gaussian distribution.
* **Median Filters**: Smoothing filters that replace each pixel with the median value of its neighborhood, effective for salt-and-pepper noise.
* **Sharpening Filters**: Filters that enhance edges and fine details in images by accentuating the differences in intensity values between neighboring pixels.
* **Laplacian Filter**: A sharpening filter that detects edges by finding areas of rapid intensity change.
* **Sobel Filters**: Sharpening filters that detect edges along specific directions.
* **Frequency Domain Filtering**: A filtering approach that transforms images into the frequency domain (e.g., using Fourier Transform) to manipulate specific frequency components for tasks like edge enhancement (high-pass) or noise reduction (low-pass).
* **Fourier Transform**: A technique used to transform images into the frequency domain for filtering.
* **Geometric Transformations**: Transformations that alter the spatial arrangement of pixels within an image, including rotation, scaling, translation, and affine transformations.
* **Rotation**: A geometric transformation that turns the image around a center point.
* **Scaling**: A geometric transformation that resizes the image.
* **Translation**: A geometric transformation that shifts the image position.
* **Affine Transformations**: Geometric transformations that are combinations of rotation, scaling, and translation that preserve parallel lines.
* **Perspective Transformation**: A technique that corrects distortions caused by viewing objects from an angle, making objects appear as if viewed from directly in front.
* **Histogram Equalization**: A contrast enhancement technique that redistributes pixel intensities to cover the entire available dynamic range, improving visibility of details in images with poor contrast.
* **CLAHE (Contrast Limited Adaptive Histogram Equalization)**: An advanced enhancement technique that performs histogram equalization in smaller regions of the image to improve local contrast without over-amplifying noise.

**Libraries Used and Their Purpose:**

* **`cv2` (OpenCV)**: A powerful open-source computer vision library used for image loading, display, resizing, color space conversion, contrast/brightness adjustment, applying various filters (Gaussian blur, median filter, bilateral filter), histogram equalization, CLAHE, sharpening, and saving images.
* **`numpy`**: A fundamental package for numerical computing in Python, used for array manipulation (e.g., creating `comparison` arrays with `np.hstack`).
* **`matplotlib.pyplot`**: A plotting library used for displaying images and visualizing results.

---

## 6.2 Object Detection

**Terms, Acronyms, and Definitions:**

* **Object Detection**: A fundamental computer vision task that enables systems to locate and identify objects within images and videos, providing both object recognition and precise localization through bounding boxes.
* **Bounding Boxes**: Rectangular boxes that precisely localize detected objects within an image or video frame.
* **CNN (Convolutional Neural Network)**: Employed in feature extraction to learn hierarchical representations of features from input images.
* **RPN (Region Proposal Network)**: A popular method for object localization that generates bounding boxes (region proposals) with confidence scores.
* **YOLO (You Only Look Once)**: A one-stage object detector that directly predicts object bounding boxes and class labels without requiring explicit region proposal generation, prioritizing speed and efficiency.
* **SSD (Single Shot MultiBox Detector)**: A one-stage object detector that directly predicts object bounding boxes and class labels by predicting at multiple feature maps of different scales, balancing speed and accuracy.
* **NMS (Non-Maximum Suppression)**: A process that removes redundant or overlapping bounding box detections, retaining only the most confident detection for each object class.
* **Two-Stage Detectors**: Object detection methods that first generate a set of region proposals (e.g., using RPNs) and then refine these proposals to detect objects, typically offering higher accuracy at the cost of computational speed.
* **Faster R-CNN**: A two-stage detector that uses RPNs to generate object proposals, followed by classification and bounding box regression.
* **R-FCN (Region-based Fully Convolutional Networks)**: A two-stage detector that shares computation for object detection across the entire image, improving efficiency over traditional R-CNN.
* **One-Stage Detectors**: Object detection methods that directly predict object bounding boxes and class labels in a single network evaluation, prioritizing speed and efficiency for real-time applications.
* **Anchor-Based Methods**: Object detection approaches that use predefined anchor boxes (default boxes or priors) of various scales and aspect ratios to generate region proposals.
* **Anchor-Free Approaches**: Object detection approaches that directly predict object bounding boxes without relying on predetermined anchors.
* **RetinaNet**: A one-stage detector that addresses class imbalance problems using focal loss.
* **Mask R-CNN**: Extends Faster R-CNN by adding a branch for predicting segmentation masks in parallel with bounding box classification and regression.
* **Cascade R-CNN**: Improves detection accuracy by cascading multiple detectors, with each stage refining the predictions of the previous stage.
* **YOLOv4**: An advanced version of YOLO incorporating various improvements for state-of-the-art performance with real-time capability.
* **CenterNet**: A one-stage detector that directly predicts object centers and sizes, achieving competitive performance with simpler architecture and faster inference.
* **Kernel**: A small matrix that slides across an image or feature map in YOLO to detect patterns.
* **Confidence Score**: A score indicating both the likelihood that a bounding box contains an object and the accuracy of the bounding box localization.
* **Multi-Attribute Prediction**: For each bounding box, YOLO predicts bounding box coordinates, a confidence score, and class probabilities.
* **Localization Loss**: A component of the YOLO loss function that uses smooth L1 loss for bounding box coordinate predictions.
* **Classification Loss**: A component of the YOLO loss function that employs binary cross-entropy loss for confidence scores and class probabilities.
* **Default Boxes (Priors)**: Predefined bounding boxes in SSD that are distributed across feature maps with various aspect ratios and scales to provide comprehensive coverage for different object characteristics.

**Libraries Used and Their Purpose:**

* **`cv2` (OpenCV)**: A powerful open-source computer vision library used for loading images, video capture, video writing, deep neural network (DNN) module for loading and running YOLO and SSD models, image preprocessing (`cv2.dnn.blobFromImage`), drawing bounding boxes and text, and displaying/saving results.
* **`numpy`**: A fundamental package for numerical computing in Python, used for array manipulation (e.g., `np.argmax`, `np.random.uniform`), and processing detection results.
* **`os`**: Python's built-in module for interacting with the operating system, used for checking file paths (`os.path.exists`).
* **`time`**: Python's built-in module for time-related functions, used for performance tracking (FPS calculation).
* **`collections.deque`**: A specialized list-like container, used for efficient appending and popping from both ends, useful for storing recent FPS values.
* **`argparse`**: Python's built-in module for parsing command-line arguments.

---

## 6.3 Image Segmentation

**Terms, Acronyms, and Definitions:**

* **Image Segmentation**: A fundamental computer vision technique that partitions an image into meaningful regions based on characteristics such as color, intensity, texture, or semantic information.
* **Semantic Segmentation**: A pixel-level classification approach where every pixel in an image is assigned to a specific class or category based on its semantic meaning, treating all objects within the same class as a unified entity.
* **Instance Segmentation**: An advanced segmentation approach that not only classifies each pixel by its semantic category but also distinguishes between different *instances* of the same object class, combining pixel-level precision with object detection capabilities.
* **U-Net Architecture**: A specialized deep learning architecture originally designed for biomedical image segmentation, known for its ability to achieve high accuracy with limited training data and its distinctive "U" shape reflecting a symmetric encoder-decoder structure.
* **Encoder (Contracting/Downsampling Path)**: The part of the U-Net architecture that progressively reduces spatial dimensions while increasing feature depth, capturing context and hierarchical representations of the input image.
* **Decoder (Expanding/Upsampling Path)**: The part of the U-Net architecture that reconstructs the spatial dimensions while maintaining learned feature representations, ultimately producing pixel-wise predictions.
* **Skip Connections**: A key innovation of U-Net that directly connects corresponding encoder and decoder layers, allowing the network to combine low-level spatial information with high-level semantic information for precise segmentation boundaries.
* **Medical Segmentation Decathlon (MSD)**: A publicly available dataset providing multi-organ segmentation challenges across different medical imaging modalities.
* **NIfTI (Neuroimaging Informatics Technology Initiative)**: A common file format for storing medical image data.
* **Z-score Normalization**: A method to normalize image intensities by subtracting the mean and dividing by the standard deviation.
* **Dice Coefficient**: A common metric used for evaluating segmentation performance, measuring the similarity between the predicted segmentation mask and the ground truth mask.
* **IoU (Intersection over Union)**: A common metric used for evaluating segmentation performance, measuring the ratio of the intersection area to the union area between the predicted segmentation mask and the ground truth mask.
* **Sensitivity (Recall)**: A metric that measures the proportion of actual positive pixels that are correctly identified.
* **Specificity**: A metric that measures the proportion of actual negative pixels that are correctly identified.
* **Quantization**: A model optimization technique that reduces model size and inference time by reducing the precision of model weights.
* **Pruning**: A model optimization technique that removes unnecessary connections or parameters from the model to reduce computational overhead.
* **Knowledge Distillation**: A model optimization technique where a smaller model is trained to mimic the outputs of a larger, more complex model.
* **TensorRT/ONNX**: Tools or formats for converting deep learning models for optimized deployment on specific hardware.
* **Edge Deployment**: Optimizing models for deployment on mobile and embedded devices with limited resources.

**Libraries Used and Their Purpose:**

* **`numpy`**: A fundamental package for numerical computing in Python, used for array manipulation (e.g., `np.mean`, `np.std`, `np.expand_dims`).
* **`cv2` (OpenCV)**: A powerful open-source computer vision library used for image resizing (`cv2.resize`) and blending images (`cv2.addWeighted`).
* **`nibabel` (nib)**: A Python library for reading and writing neuroimaging file formats like NIfTI.
* **`sklearn.model_selection.train_test_split`**: Used to split datasets into training and testing (or validation) sets.
* **`tensorflow`**: A comprehensive open-source machine learning platform for building and training neural networks.
* **`tensorflow.keras.utils.Sequence`**: A base class for Keras data generators, used to create custom dataset classes for efficient batch loading (e.g., `MedicalImageDataset`).
* **`albumentations` (A)**: A fast and flexible image augmentation library, used for applying various transformations to images and masks.
* **`tensorflow.keras.layers`**: Provides various layers for building neural network architectures (e.g., `Conv2D`, `MaxPooling2D`, `Conv2DTranspose`, `concatenate`, `BatchNormalization`, `Dropout`, `ReLU`, `SeparableConv2D`).
* **`tensorflow.keras.models`**: Provides tools for creating neural network models (e.g., `Sequential`, `Model`, `Input`).
* **`tensorflow.keras.optimizers.Adam`**: An optimizer used during model compilation for efficient training.
* **`tensorflow.keras.losses.BinaryCrossentropy`**: A loss function used for binary classification tasks.
* **`tensorflow.keras.backend` (K)**: Provides backend-agnostic functions for common operations, used here to implement custom metrics like Dice coefficient.
* **`tensorflow.keras.callbacks`**: Provides tools for controlling model training (e.g., `ModelCheckpoint`, `ReduceLROnPlateau`, `EarlyStopping`).
* **`matplotlib.pyplot`**: A plotting library used for visualizing training history and prediction results.
* **`sklearn.metrics.classification_report`**: Used to generate detailed classification reports (though not directly used for per-pixel metrics here, it's mentioned in a general evaluation context).
* **`seaborn`**: A data visualization library based on matplotlib, used for creating statistical graphics.

---

## 6.4 Generative Adversarial Networks (GANs)

**Terms, Acronyms, and Definitions:**

* **GANs (Generative Adversarial Networks)**: A revolutionary deep learning framework introduced by Ian Goodfellow in 2014, employing an adversarial training process between two neural networks – a generator and a discriminator – to generate synthetic data.
* **Generator Network**: The creative component of a GAN system that takes random noise vectors as input and transforms them into synthetic data samples that aim to be indistinguishable from real data.
* **Discriminator Network**: The critic or evaluator within a GAN framework, trained to distinguish between authentic data samples from the training dataset and synthetic samples generated by the generator.
* **Adversarial Training Process**: The core mechanism of GANs involving a competitive process where the generator attempts to create realistic synthetic data while the discriminator becomes progressively better at distinguishing between real and generated samples.
* **Latent Space**: The typically low-dimensional space from which random noise vectors are sampled as input to the generator (often from a Gaussian or uniform distribution).
* **Transposed Convolutions (Deconvolutional Layers)**: Layers typically employed by the generator architecture to progressively upsample the input noise vector into full-resolution synthetic samples.
* **Generator Training Phase**: The phase where the generator focuses on improving its ability to create realistic synthetic data, receiving feedback from the discriminator's classification decisions.
* **Discriminator Training Phase**: The phase where the discriminator is trained using both authentic data samples and synthetic samples, learning to maximize its accuracy in distinguishing between the two.
* **Mode Collapse**: A challenge in GANs where the generator learns to produce only a limited variety of samples, failing to capture the full diversity of the target data distribution.
* **cGANs (Conditional GANs)**: Extend the basic GAN framework by conditioning both the generator and discriminator on additional information (e.g., class labels, attributes), enabling controlled generation.
* **DCGANs (Deep Convolutional GANs)**: A significant architectural advancement that leverages deep convolutional neural networks in both generator and discriminator components, improving training stability and quality.
* **WGANs (Wasserstein GANs)**: Address fundamental training challenges in traditional GANs by introducing the Wasserstein distance (Earth Mover's Distance) as an alternative objective function, providing more meaningful loss metrics and improved training dynamics.
* **Progressive GANs**: Introduce a novel training strategy that begins with low-resolution image generation and progressively increases resolution during training, enabling high-quality, high-resolution image generation.
* **StyleGAN Architecture**: A significant advancement in controllable image generation, introducing a style-based generator architecture that enables fine-grained control over generated image characteristics.
* **Inception Score (IS)**: A metric developed to assess the quality and diversity of generated samples in GANs.
* **FID (Fréchet Inception Distance)**: A metric used to assess the quality and diversity of generated samples, considered more robust than Inception Score.
* **PPL (Perceptual Path Length)**: A metric used to evaluate the linearity of interpolations in the latent space of GANs, indicating smoothness of generated samples.
* **Label Smoothing**: A technique used in GAN training (e.g., in DCGAN compilation) where target labels are slightly perturbed from 0 or 1 to improve training stability.
* **TTUR (Two-Times Update Rule)**: A training strategy where the generator and discriminator are updated with different learning rates (e.g., discriminator with a slightly higher learning rate).
* **PatchGAN Discriminator**: A type of discriminator (used in CycleGAN) that classifies image patches as real or fake, rather than the entire image, to encourage high-frequency detail preservation.
* **Cycle Consistency Loss**: A loss function in CycleGAN that ensures that if an image is translated from one domain to another and then back, it should resemble the original image.
* **Identity Loss**: A loss function in CycleGAN that encourages the generator to preserve color composition when mapping an image to itself (e.g., generator A-to-B should produce the same image if fed an image from domain B).

**Libraries Used and Their Purpose:**

* **`os`**: Python's built-in module for interacting with the operating system, used for file system operations.
* **`numpy`**: A fundamental package for numerical computing in Python, used for array manipulation (e.g., `np.array`, `np.log`, `np.exp`).
* **`tensorflow`**: A comprehensive open-source machine learning platform for building and training neural networks.
* **`tensorflow.keras.layers`**: Provides layers for building neural network architectures (e.g., `Dense`, `BatchNormalization`, `ReLU`, `Reshape`, `Conv2D`, `UpSampling2D`, `Add`, `LeakyReLU`, `Dropout`, `GlobalAveragePooling2D`, `Conv2DTranspose`, `concatenate`, `SeparableConv2D`).
* **`tensorflow.keras.models`**: Provides tools for creating neural network models (e.g., `Model`, `Sequential`, `Input`).
* **`tensorflow.keras.optimizers.Adam`**: An optimizer used during model compilation for efficient training.
* **`tensorflow.keras.callbacks`**: Provides tools for controlling model training (e.g., `ModelCheckpoint`, `ReduceLROnPlateau`, `EarlyStopping`, although not all are explicitly used in the GAN training loops, they are part of the Keras ecosystem).
* **`matplotlib.pyplot`**: A plotting library used for visualizing generated images and training loss history.
* **`cv2` (OpenCV)**: A powerful open-source computer vision library used for image loading (`cv2.imread`), color space conversion (`cv2.cvtColor`), and resizing (`cv2.resize`) in the data preprocessing pipeline.
* **`sklearn.model_selection.train_test_split`**: Used to split datasets into training, validation, and test sets.
* **`albumentations` (A)**: A fast and flexible image augmentation library, used for applying various transformations to images.
* **`pathlib.Path`**: A Python module providing an object-oriented way to interact with file system paths, used for loading image paths.
* **`random`**: Python's built-in module for generating random numbers, used for shuffling examples in custom NER training.
* **`scipy.linalg.sqrtm`**: Used for matrix square root calculation in FID (Fréchet Inception Distance) calculation.
* **`scipy.stats.entropy`**: Used to calculate Kullback-Leibler divergence for Inception Score.
