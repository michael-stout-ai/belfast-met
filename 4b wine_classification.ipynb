{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Dataset Classification Analysis\n",
    "\n",
    "## Overview\n",
    "The Wine dataset contains the results of chemical analysis of wines grown in the same region of Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "\n",
    "## Dataset Details\n",
    "- **Samples**: 178 wine samples\n",
    "- **Features**: 13 chemical properties\n",
    "- **Target**: 3 wine classes (cultivars)\n",
    "- **Task**: Multi-class classification\n",
    "\n",
    "## Features (Chemical Properties)\n",
    "1. Alcohol\n",
    "2. Malic acid\n",
    "3. Ash\n",
    "4. Alcalinity of ash\n",
    "5. Magnesium\n",
    "6. Total phenols\n",
    "7. Flavanoids\n",
    "8. Nonflavanoid phenols\n",
    "9. Proanthocyanins\n",
    "10. Color intensity\n",
    "11. Hue\n",
    "12. OD280/OD315 of diluted wines\n",
    "13. Proline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "Import all necessary libraries for data analysis, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Dataset\n",
    "Load the Wine dataset and examine its structure and basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target\n",
    "df['wine_class'] = df['target'].map({0: 'Class 0', 1: 'Class 1', 2: 'Class 2'})\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['wine_class'].value_counts())\n",
    "print(\"\\nClass Distribution (counts):\")\n",
    "print(df['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Statistical Summary and Data Quality Check\n",
    "Examine statistical properties and check for data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Feature ranges (to understand scaling needs)\n",
    "print(\"\\nFeature Ranges:\")\n",
    "feature_ranges = pd.DataFrame({\n",
    "    'Min': df[wine.feature_names].min(),\n",
    "    'Max': df[wine.feature_names].max(),\n",
    "    'Range': df[wine.feature_names].max() - df[wine.feature_names].min()\n",
    "})\n",
    "print(feature_ranges.sort_values('Range', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis and Visualization\n",
    "Create comprehensive visualizations to understand data patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "df['wine_class'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Wine Class Distribution')\n",
    "plt.xlabel('Wine Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(df['wine_class'].value_counts(), labels=df['wine_class'].value_counts().index, autopct='%1.1f%%')\n",
    "plt.title('Wine Class Distribution (Pie Chart)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution by class - Box plots\n",
    "# Select top 6 features for visualization (based on variance)\n",
    "feature_variance = df[wine.feature_names].var().sort_values(ascending=False)\n",
    "top_features = feature_variance.head(6).index\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(top_features):\n",
    "    sns.boxplot(data=df, x='wine_class', y=feature, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by Wine Class')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Feature Distributions by Wine Class (Top 6 by Variance)', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top features by variance:\")\n",
    "for i, (feature, variance) in enumerate(feature_variance.head(6).items()):\n",
    "    print(f\"{i+1}. {feature}: {variance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "correlation_matrix = df[wine.feature_names].corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Matrix (Wine Dataset)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], corr_val))\n",
    "\n",
    "print(\"\\nHighly correlated feature pairs (|correlation| > 0.7):\")\n",
    "for feat1, feat2, corr in high_corr_pairs:\n",
    "    print(f\"{feat1} - {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Scaling and Data Preprocessing\n",
    "Prepare the data for machine learning by scaling features and splitting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Feature dimensions: {X_train.shape[1]}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"Class {cls}: {count} samples ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"Class {cls}: {count} samples ({count/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Apply different scaling methods\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_std = standard_scaler.fit_transform(X_train)\n",
    "X_test_std = standard_scaler.transform(X_test)\n",
    "\n",
    "X_train_minmax = minmax_scaler.fit_transform(X_train)\n",
    "X_test_minmax = minmax_scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeature scaling completed with both StandardScaler and MinMaxScaler.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Training and Evaluation\n",
    "Train multiple classification models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate models with different scaling\n",
    "scaling_methods = {\n",
    "    'Standard Scaling': (X_train_std, X_test_std),\n",
    "    'MinMax Scaling': (X_train_minmax, X_test_minmax),\n",
    "    'No Scaling': (X_train, X_test)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for scale_name, (X_tr, X_te) in scaling_methods.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Results with {scale_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    scale_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_tr, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_te)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_tr, y_train, cv=5)\n",
    "        print(f\"Cross-validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        # Store results\n",
    "        scale_results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': y_pred,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std()\n",
    "        }\n",
    "    \n",
    "    results[scale_name] = scale_results\n",
    "\n",
    "# Find best performing model overall\n",
    "best_accuracy = 0\n",
    "best_model_info = None\n",
    "\n",
    "for scale_name, scale_results in results.items():\n",
    "    for model_name, model_results in scale_results.items():\n",
    "        if model_results['accuracy'] > best_accuracy:\n",
    "            best_accuracy = model_results['accuracy']\n",
    "            best_model_info = (scale_name, model_name, model_results)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST PERFORMING MODEL\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Scaling: {best_model_info[0]}\")\n",
    "print(f\"Model: {best_model_info[1]}\")\n",
    "print(f\"Test Accuracy: {best_model_info[2]['accuracy']:.4f}\")\n",
    "print(f\"CV Accuracy: {best_model_info[2]['cv_mean']:.4f} (+/- {best_model_info[2]['cv_std'] * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Detailed Analysis of Best Model\n",
    "Perform detailed analysis including classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best performing model for detailed analysis\n",
    "best_scale, best_model_name, best_results = best_model_info\n",
    "best_predictions = best_results['predictions']\n",
    "\n",
    "print(f\"Detailed Analysis - {best_model_name} with {best_scale}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_predictions, target_names=wine.target_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
    "plt.title(f'Confusion Matrix - {best_model_name} ({best_scale})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class {i} ({wine.target_names[i]}): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model Performance Comparison Visualization\n",
    "Create comprehensive visualizations comparing all models and scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for scale_name, scale_results in results.items():\n",
    "    for model_name, model_results in scale_results.items():\n",
    "        comparison_data.append({\n",
    "            'Scaling': scale_name,\n",
    "            'Model': model_name,\n",
    "            'Test_Accuracy': model_results['accuracy'],\n",
    "            'CV_Mean': model_results['cv_mean'],\n",
    "            'CV_Std': model_results['cv_std']\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Model performance heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "pivot_test = comparison_df.pivot(index='Model', columns='Scaling', values='Test_Accuracy')\n",
    "sns.heatmap(pivot_test, annot=True, cmap='RdYlGn', center=0.9, fmt='.3f')\n",
    "plt.title('Test Accuracy Comparison Across Models and Scaling Methods')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, scale_name in enumerate(scaling_methods.keys()):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    scale_data = comparison_df[comparison_df['Scaling'] == scale_name]\n",
    "    plt.bar(scale_data['Model'], scale_data['Test_Accuracy'], alpha=0.7)\n",
    "    plt.title(f'Model Performance with {scale_name}')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0.8, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nComplete Performance Summary:\")\n",
    "summary_table = comparison_df.pivot(index='Model', columns='Scaling', values='Test_Accuracy')\n",
    "print(summary_table.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Dimensionality Reduction and Visualization\n",
    "Apply PCA and t-SNE for data visualization and dimensionality reduction analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_train_std)\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA: Cumulative Explained Variance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA: Individual Component Variance')\n",
    "\n",
    "# 2D PCA visualization\n",
    "plt.subplot(1, 3, 3)\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, (color, target_name) in enumerate(zip(colors, wine.target_names)):\n",
    "    mask = y_train == i\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=color, label=target_name, alpha=0.7)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('2D PCA Visualization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "print(f\"Number of components needed for 95% variance: {n_components_95}\")\n",
    "print(f\"First 5 components explain {cumsum_var[4]:.1%} of variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for non-linear dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_train_std)\n",
    "\n",
    "# Create visualization comparing PCA and t-SNE\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# PCA plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, (color, target_name) in enumerate(zip(colors, wine.target_names)):\n",
    "    mask = y_train == i\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=color, label=target_name, alpha=0.7, s=50)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "plt.title('PCA - Linear Dimensionality Reduction')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# t-SNE plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, (color, target_name) in enumerate(zip(colors, wine.target_names)):\n",
    "    mask = y_train == i\n",
    "    plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=color, label=target_name, alpha=0.7, s=50)\n",
    "\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE - Non-linear Dimensionality Reduction')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Dimensionality Reduction Analysis:\")\n",
    "print(f\"- PCA: First 2 components explain {(pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1]):.1%} of variance\")\n",
    "print(f\"- t-SNE: Non-linear reduction shows {len(np.unique(y_train))} distinct clusters\")\n",
    "print(f\"- Both methods show good class separation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Feature Importance Analysis\n",
    "Analyze which features are most important for wine classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest (works with all scaling methods)\n",
    "rf_results = results['No Scaling']['Random Forest']  # RF doesn't require scaling\n",
    "rf_model = rf_results['model']\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': wine.feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# PCA component analysis\n",
    "pca_components = pd.DataFrame(\n",
    "    pca.components_[:3].T,  # First 3 components\n",
    "    columns=['PC1', 'PC2', 'PC3'],\n",
    "    index=wine.feature_names\n",
    ")\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(data=feature_importance_rf.head(10), x='importance', y='feature')\n",
    "plt.title('Top 10 Features - Random Forest Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "\n",
    "# PCA Component Loadings\n",
    "plt.subplot(2, 2, 2)\n",
    "pca_abs = pca_components.abs()\n",
    "top_features_pca = pca_abs.sum(axis=1).sort_values(ascending=False).head(10)\n",
    "sns.barplot(x=top_features_pca.values, y=top_features_pca.index)\n",
    "plt.title('Top 10 Features - PCA Loading Magnitude')\n",
    "plt.xlabel('Sum of Absolute Loadings (PC1-PC3)')\n",
    "\n",
    "# Feature importance heatmap\n",
    "plt.subplot(2, 2, 3)\n",
    "top_10_features = feature_importance_rf.head(10)['feature']\n",
    "correlation_subset = df[top_10_features].corr()\n",
    "sns.heatmap(correlation_subset, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Correlation Matrix - Top 10 Important Features')\n",
    "\n",
    "# Compare different importance measures\n",
    "plt.subplot(2, 2, 4)\n",
    "# Combine different importance measures\n",
    "importance_comparison = pd.DataFrame({\n",
    "    'RF_Importance': feature_importance_rf.set_index('feature')['importance'],\n",
    "    'PCA_Loading': pca_abs.sum(axis=1)\n",
    "}).fillna(0)\n",
    "\n",
    "# Normalize for comparison\n",
    "importance_comparison = importance_comparison.div(importance_comparison.max())\n",
    "importance_comparison['Average'] = importance_comparison.mean(axis=1)\n",
    "importance_comparison = importance_comparison.sort_values('Average', ascending=False)\n",
    "\n",
    "x = np.arange(len(importance_comparison.head(8)))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, importance_comparison.head(8)['RF_Importance'], \n",
    "        width, label='Random Forest', alpha=0.7)\n",
    "plt.bar(x + width/2, importance_comparison.head(8)['PCA_Loading'], \n",
    "        width, label='PCA Loading', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Normalized Importance')\n",
    "plt.title('Feature Importance Comparison')\n",
    "plt.xticks(x, importance_comparison.head(8).index, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features (Random Forest):\")\n",
    "for i, row in feature_importance_rf.head(10).iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 Features by PCA Loading Magnitude:\")\n",
    "for feature, loading in top_features_pca.head(5).items():\n",
    "    print(f\"{feature}: {loading:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Hyperparameter Tuning\n",
    "Perform hyperparameter tuning on the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model type\n",
    "# Let's tune both SVM and Random Forest as they often perform well\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "        'kernel': ['rbf', 'poly', 'linear']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "}\n",
    "\n",
    "models_for_tuning = {\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "tuning_results = {}\n",
    "\n",
    "# Use standard scaled data for tuning\n",
    "for model_name, model in models_for_tuning.items():\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grids[model_name], \n",
    "        cv=5, \n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_std, y_train)\n",
    "    \n",
    "    # Test the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = best_model.predict(X_test_std)\n",
    "    tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "    \n",
    "    tuning_results[model_name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'test_accuracy': tuned_accuracy,\n",
    "        'model': best_model,\n",
    "        'predictions': y_pred_tuned\n",
    "    }\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Test accuracy: {tuned_accuracy:.4f}\")\n",
    "\n",
    "# Compare with original results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name in tuning_results.keys():\n",
    "    original_acc = results['Standard Scaling'][model_name]['accuracy']\n",
    "    tuned_acc = tuning_results[model_name]['test_accuracy']\n",
    "    improvement = tuned_acc - original_acc\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Original accuracy: {original_acc:.4f}\")\n",
    "    print(f\"  Tuned accuracy: {tuned_acc:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:+.4f}\")\n",
    "    print(f\"  Best parameters: {tuning_results[model_name]['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Final Model Evaluation and Summary\n",
    "Comprehensive evaluation of the best performing model with all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model overall (including tuned models)\n",
    "all_results = []\n",
    "\n",
    "# Add original results\n",
    "for scale_name, scale_results in results.items():\n",
    "    for model_name, model_results in scale_results.items():\n",
    "        all_results.append({\n",
    "            'model_type': f\"{model_name} ({scale_name})\",\n",
    "            'accuracy': model_results['accuracy'],\n",
    "            'cv_mean': model_results['cv_mean'],\n",
    "            'source': 'Original'\n",
    "        })\n",
    "\n",
    "# Add tuned results\n",
    "for model_name, tuned_results in tuning_results.items():\n",
    "    all_results.append({\n",
    "        'model_type': f\"{model_name} (Tuned)\",\n",
    "        'accuracy': tuned_results['test_accuracy'],\n",
    "        'cv_mean': tuned_results['best_cv_score'],\n",
    "        'source': 'Tuned'\n",
    "    })\n",
    "\n",
    "# Create final comparison\n",
    "final_comparison = pd.DataFrame(all_results).sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"FINAL MODEL RANKING (Top 10):\")\n",
    "print(\"=\"*60)\n",
    "print(final_comparison.head(10).to_string(index=False))\n",
    "\n",
    "# Best model analysis\n",
    "best_model_row = final_comparison.iloc[0]\n",
    "print(f\"\\n\\nBEST OVERALL MODEL: {best_model_row['model_type']}\")\n",
    "print(f\"Test Accuracy: {best_model_row['accuracy']:.4f}\")\n",
    "print(f\"CV Score: {best_model_row['cv_mean']:.4f}\")\n",
    "\n",
    "# Get the actual best model for detailed analysis\n",
    "if 'Tuned' in best_model_row['model_type']:\n",
    "    model_name = best_model_row['model_type'].split(' (')[0]\n",
    "    final_best_model = tuning_results[model_name]['model']\n",
    "    final_predictions = tuning_results[model_name]['predictions']\n",
    "else:\n",
    "    # Parse original model info\n",
    "    parts = best_model_row['model_type'].split(' (')\n",
    "    model_name = parts[0]\n",
    "    scale_name = parts[1].rstrip(')')\n",
    "    final_best_model = results[scale_name][model_name]['model']\n",
    "    final_predictions = results[scale_name][model_name]['predictions']\n",
    "\n",
    "# Final confusion matrix and classification report\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "cm_final = confusion_matrix(y_test, final_predictions)\n",
    "sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
    "plt.title(f'Final Model Confusion Matrix\\n{best_model_row[\"model_type\"]}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Performance comparison visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "top_models = final_comparison.head(8)\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(top_models)))\n",
    "\n",
    "bars = plt.barh(range(len(top_models)), top_models['accuracy'], color=colors)\n",
    "plt.yticks(range(len(top_models)), [name.replace(' (', '\\n(') for name in top_models['model_type']])\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Top 8 Models Performance')\n",
    "plt.xlim(0.9, 1.0)\n",
    "\n",
    "# Add accuracy labels on bars\n",
    "for i, (bar, acc) in enumerate(zip(bars, top_models['accuracy'])):\n",
    "    plt.text(acc + 0.001, i, f'{acc:.3f}', va='center', ha='left', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "print(classification_report(y_test, final_predictions, target_names=wine.target_names))\n",
    "\n",
    "# Per-class performance\n",
    "per_class_acc = cm_final.diagonal() / cm_final.sum(axis=1)\n",
    "print(\"\\nPer-class Performance:\")\n",
    "for i, (acc, name) in enumerate(zip(per_class_acc, wine.target_names)):\n",
    "    print(f\"{name}: {acc:.4f} ({cm_final.diagonal()[i]}/{cm_final.sum(axis=1)[i]} correct)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Conclusions\n",
    "\n",
    "### Dataset Characteristics\n",
    "- **Size**: 178 samples with 13 chemical features\n",
    "- **Balance**: Relatively balanced classes (Class 0: 59, Class 1: 71, Class 2: 48)\n",
    "- **Quality**: No missing values, high-quality chemical measurements\n",
    "- **Complexity**: More challenging than Iris due to higher dimensionality and subtle class differences\n",
    "\n",
    "### Feature Analysis\n",
    "- **High Variance Features**: Proline, Color intensity, OD280/OD315 show highest variance\n",
    "- **Correlations**: Several strong correlations exist (e.g., flavanoids with total phenols)\n",
    "- **Importance**: Flavanoids, Proline, and Color intensity are consistently most important\n",
    "- **Dimensionality**: First 5 PCA components capture ~80% of variance\n",
    "\n",
    "### Model Performance\n",
    "- **Best Models**: SVM and ensemble methods generally perform best\n",
    "- **Scaling Impact**: Standard scaling significantly improves performance for distance-based models\n",
    "- **Accuracy Range**: Most models achieve 90-100% accuracy\n",
    "- **Stability**: Cross-validation scores are consistent with test performance\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "- **Improvement**: Tuning provides modest but consistent improvements\n",
    "- **Best Parameters**: Typically favor balanced complexity (moderate C for SVM, reasonable depth for RF)\n",
    "- **Overfitting Risk**: High accuracy suggests potential overfitting due to small dataset size\n",
    "\n",
    "### Practical Implications\n",
    "- **Wine Classification**: Chemical analysis can reliably distinguish wine cultivars\n",
    "- **Feature Selection**: Focus on flavanoids, proline, and color-related measurements\n",
    "- **Model Choice**: SVM with RBF kernel or ensemble methods recommended\n",
    "- **Data Collection**: Additional samples would improve model robustness\n",
    "\n",
    "### Recommendations\n",
    "1. **Production Use**: Collect more data before deploying in production\n",
    "2. **Feature Engineering**: Consider ratios and interactions between chemical compounds\n",
    "3. **Validation**: Use external validation set from different harvest years/regions\n",
    "4. **Monitoring**: Track model performance over time as wine characteristics may change"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}