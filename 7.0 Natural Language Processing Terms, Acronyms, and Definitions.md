## 7.1 Text Preprocessing and Representation

**Terms, Acronyms, and Definitions:**

* **NLP (Natural Language Processing)**: A field of artificial intelligence that involves preparing raw text data for analysis and modeling.
* **Text Preprocessing**: The crucial step of cleaning and standardizing raw text data to transform unstructured, messy text into a consistent format suitable for machine learning algorithms.
* **Text Representation**: The process of converting preprocessed text into numerical representations that machine learning algorithms can process effectively.
* **Text Cleaning**: The process of removing noise and irrelevant elements from raw text, such as HTML tags, special characters, punctuation marks, and non-alphanumeric symbols.
* **Noise Removal**: The act of eliminating irrelevant characters and symbols from text that do not contribute to its meaning.
* **Case Normalization**: Converting text to lowercase for uniformity to prevent word duplication based on case differences (e.g., "Apple" vs "apple").
* **Spell Correction**: Correcting spelling mistakes using dictionaries or spell-checking algorithms to improve text quality and consistency.
* **Contraction Handling**: Expanding contractions (e.g., "can't" → "cannot") to ensure uniformity in text representation.
* **Tokenization**: Breaking text into smaller, manageable units.
* **Word Tokenization**: Breaking down text into individual words or tokens.
* **Sentence Tokenization**: Splitting text into individual sentences, essential for tasks such as sentiment analysis or text summarization.
* **Stopword Removal**: Filtering out common words that add little semantic value.
* **Stopwords**: Commonly occurring words (e.g., "and", "the", "is") that often do not carry significant meaning in text analysis and can be safely removed.
* **Text Normalization**: Techniques that reduce words to their base forms.
* **Stemming**: Removing suffixes from words to obtain their root form (e.g., "running" → "run"); may produce non-dictionary words.
* **Lemmatization**: Converting words to their base or dictionary form while considering context (e.g., "better" → "good"); produces valid dictionary words.
* **BoW (Bag-of-Words) Model**: A simple numerical representation where text documents are represented as vectors with each dimension corresponding to a unique word, and the value represents the frequency of that word in the document, ignoring word order.
* **Count Vectorization**: A method within BoW that represents documents as vectors where each dimension is a unique word, and the value is its frequency; ignores word order.
* **TF-IDF (Term Frequency-Inverse Document Frequency)**: An extension of count vectorization where word frequencies are normalized by their inverse frequency across the corpus, emphasizing rare but informative terms.
* **Word Embeddings**: Dense, low-dimensional vector representations of words in a continuous vector space, capturing semantic relationships based on context.
* **Word2Vec**: A technique that represents words as dense, low-dimensional vectors in a continuous vector space, capturing semantic relationships based on context, trained by neural networks.
* **GloVe (Global Vectors for Word Representation)**: Pre-trained word embeddings that capture global word co-occurrence statistics from large text corpora.
* **Contextual Embeddings**: Advanced word embeddings that consider the context in which words appear.
* **ELMo (Embeddings from Language Models)**: Deep contextual word embeddings that capture the contextual meaning of words within sentences, generated by pre-trained language models.
* **BERT (Bidirectional Encoder Representations from Transformers)**: State-of-the-art pre-trained language models that generate contextual embeddings for words, sentences, or entire documents, considering both left and right context.

**Libraries Used and Their Purpose:**

* **`nltk` (Natural Language Toolkit)**: A popular Python library for working with human language data, used for tokenization (`nltk.word_tokenize`), accessing stopwords (`nltk.corpus.stopwords`), and other text processing tasks.
* **`nltk.corpus.stopwords`**: Provides a list of common stopwords.
* **`re` (Regular Expressions)**: Python's built-in module for working with regular expressions, used for text cleaning operations like removing punctuation and numbers.

---

## 7.2 Text Classification

**Terms, Acronyms, and Definitions:**

* **Text Classification**: A fundamental Natural Language Processing (NLP) task that involves automatically categorizing text documents based on their content, assigning predetermined labels or classes to text inputs.
* **Text Data**: A corpus of documents categorized into specified classes, serving as the input for text classification systems.
* **Preprocessing**: The stage where raw text data is cleaned, tokenized, and normalized before being used to train a text classifier.
* **Feature Extraction**: The process that transforms text data into numerical representations that machine learning algorithms can process effectively.
* **BoW (Bag-of-Words)**: A feature extraction method that represents text documents as vectors where each dimension corresponds to a unique word, and the value represents the frequency of that word in the document.
* **TF-IDF (Term Frequency-Inverse Document Frequency)**: A feature extraction method that normalizes word frequencies by their frequency across all documents in the corpus, reducing the importance of common words.
* **Model Building**: The process of using machine learning or deep learning algorithms to train text classification models on labeled data.
* **SVM (Support Vector Machines)**: A traditional machine learning algorithm used in text classification.
* **CNN (Convolutional Neural Network)**: A modern deep learning method used for complex text classification tasks.
* **RNN (Recurrent Neural Network)**: A modern deep learning method used for complex text classification tasks.
* **Transformers**: Modern deep learning architectures that have become increasingly popular for complex text classification tasks.
* **Model Evaluation**: The process of assessing the performance of a trained text classification model on a separate test dataset.
* **Accuracy**: A common evaluation metric for text classification.
* **Precision**: A common evaluation metric for text classification.
* **Recall**: A common evaluation metric for text classification.
* **F1-score**: A common evaluation metric for text classification that is the harmonic mean of precision and recall.
* **Confusion Matrix**: A table used in model evaluation to visualize the performance of a classification model.
* **Cross-validation**: A technique used to verify model performance across different data subsets, ensuring robustness and generalizability.
* **Data Imbalance**: An issue in text classification where there is an unequal distribution of data across classes, which can lead to biased models.
* **Sentiment Analysis**: An NLP technique, also known as opinion mining, used to assess the emotional tone of text, classifying it as positive, negative, or neutral.
* **LDA (Latent Dirichlet Allocation)**: A popular statistical modeling approach for topic modeling, assuming that each document represents a mixture of multiple topics, and each topic is characterized by a probability distribution over vocabulary words.
* **Topic Modeling**: An advanced NLP technique that discovers latent themes within collections of text documents without requiring prior knowledge of the content structure.
* **Spam Detection**: A specialized application of text classification focused on identifying and filtering unwanted or malicious emails, messages, and comments.

**Libraries Used and Their Purpose:**

* **`pandas`**: A library for data manipulation and analysis, primarily used for creating and managing DataFrames to store text data and sentiment labels.
* **`numpy`**: A fundamental package for numerical computing in Python, used for numerical operations and array handling.
* **`sklearn.model_selection.train_test_split`**: Used to split datasets into training and testing subsets.
* **`sklearn.feature_extraction.text.TfidfVectorizer`**: Used for TF-IDF feature extraction, converting raw text documents into a matrix of TF-IDF features.
* **`sklearn.linear_model.LogisticRegression`**: A machine learning model used for classification tasks, trained here for sentiment analysis.
* **`sklearn.naive_bayes.MultinomialNB`**: A Naive Bayes classifier used for spam detection in the example.
* **`sklearn.metrics.accuracy_score`**: Used to calculate the accuracy of classification predictions.
* **`sklearn.metrics.classification_report`**: Used to generate a detailed report of precision, recall, and F1-score for each class.
* **`sklearn.metrics.confusion_matrix`**: Used to compute the confusion matrix, visualizing the performance of the classification model.
* **`re` (Regular Expressions)**: Python's built-in module for working with regular expressions, used for text cleaning (e.g., removing URLs, mentions, special characters).
* **`nltk` (Natural Language Toolkit)**: A popular Python library for working with human language data, used for tokenization (`nltk.tokenize.word_tokenize`) and accessing stopwords (`nltk.corpus.stopwords`).
* **`nltk.corpus.stopwords`**: Provides a list of common stopwords in various languages.
* **`nltk.tokenize.word_tokenize`**: Used to split text into individual words or tokens.
* **`nltk.corpus.movie_reviews`**: A dataset from NLTK used for sentiment classification examples.
* **`nltk.classify.NaiveBayesClassifier`**: A classifier from NLTK used in the sentiment analysis example.
* **`nltk.classify.util.accuracy`**: A utility from NLTK to calculate classifier accuracy.
* **`random.shuffle`**: Used to shuffle combined datasets in the sentiment analysis example.
* **`gensim`**: A Python library for topic modeling and document similarity analysis, used specifically for LDA.
* **`gensim.corpora`**: Used to create dictionaries and corpora for topic modeling with Gensim.
* **`gensim.models.LdaModel`**: The implementation of Latent Dirichlet Allocation within Gensim.
* **`nltk.stem.WordNetLemmatizer`**: Used for lemmatization in text preprocessing for topic modeling.

---

## 7.3 Named Entity Recognition (NER)

**Terms, Acronyms, and Definitions:**

* **NER (Named Entity Recognition)**: A fundamental Natural Language Processing (NLP) technique that identifies and classifies named entities (e.g., persons, organizations, locations, dates) within unstructured text data.
* **Named Entities**: Specific entities like persons, organizations, locations, dates, and other meaningful information identified and classified by NER.
* **Text Data**: Diverse text corpora (e.g., articles, news, social media, legal documents) on which NER operates.
* **Preprocessing**: The systematic cleaning and preparation of text data before performing NER, including tokenization, noise removal, punctuation handling, stopword management, and text normalization.
* **Named Entity Recognition Models**: Computational approaches (ML algorithms, deep learning architectures, rule-based systems) used to identify and classify named entities, trained on annotated datasets.
* **Feature Extraction**: The process of deriving meaningful representations from text data to capture the context surrounding each word or token for NER.
* **Entity Classification**: The process by which NER models classify each word or token sequence into predefined categories representing distinct named entity types.
* **Person (PERSON)**: An entity type representing names of individuals, fictional characters, titles, and honorifics.
* **Organization (ORG)**: An entity type representing companies, institutions, government agencies, and formal groups.
* **Location (GPE/LOC)**: An entity type representing geographic entities including countries, cities, regions, and landmarks.
* **Date (DATE)**: An entity type representing temporal expressions for specific dates, times, or durations.
* **Money (MONEY)**: An entity type representing monetary values, currencies, and financial amounts.
* **Miscellaneous Entities**: Specialized categories like product names, events, numerical quantities, laws, or works of art.
* **Entity Labeling**: The process of assigning specific labels or tags to each recognized entity, indicating its category and boundaries within the text.
* **Rule-Based NER Systems**: Approaches that use manually crafted rules and patterns (e.g., regular expressions, grammatical rules) to recognize entities.
* **Statistical Models**: NER models that leverage machine learning techniques (e.g., CRF, HMM, MEMM) to identify patterns and relationships in text data from labeled training datasets.
* **CRF (Conditional Random Fields)**: A statistical model used in NER to capture global label dependencies.
* **HMM (Hidden Markov Models)**: A statistical model used in NER to capture sequential dependencies in entity labeling.
* **MEMM (Maximum Entropy Markov Models)**: A statistical model used in NER.
* **Deep Learning Architectures**: Modern NER systems employing sophisticated neural network architectures like RNNs, LSTMs, and Transformer models (BERT, GPT) to capture complex contextual dependencies.
* **RNN (Recurrent Neural Network)**: A neural network architecture used in deep learning NER systems.
* **LSTM (Long Short-Term Memory) Network**: A type of RNN used in deep learning NER systems.
* **Transformer Models**: Deep learning models such as BERT and GPT that capture bidirectional context and achieve state-of-the-art performance in NER.
* **BERT (Bidirectional Encoder Representations from Transformers)**: A Transformer model used in deep learning NER.
* **GPT (Generative Pre-trained Transformer)**: A Transformer model used in deep learning NER.
* **Information Extraction**: A general application of NER that systematically extracts structured information from unstructured text data.
* **Question Answering Systems**: Applications where NER identifies relevant entities in queries and documents for accurate retrieval and answer generation.
* **Document Summarization**: Applications where NER assists by identifying key entities and events to generate concise summaries.
* **Entity Linking**: Tasks that connect recognized entities to knowledge bases or databases, enriching their semantic meaning.
* **Knowledge Graphs**: Structured representations of information that use entities and relationships, often built with the help of NER and entity linking.
* **Named Entity Disambiguation**: Resolves ambiguities in entity mentions by distinguishing between entities with similar names or overlapping contexts.
* **BIO Tagging Scheme**: An entity labeling scheme using Beginning-Inside-Outside notation (B- for beginning, I- for inside, O for outside).
* **BILOU Tagging**: An extended entity labeling scheme including B-, I-, L- (Last token), and U- (Unit-length entity).
* **Entity Boundary Detection**: The process of identifying precise start and end positions of entities, handling multi-word and nested entities.
* **Entity Normalization**: Standardizes entity representations (e.g., "U.S." and "United States" to consistent forms).
* **Confidence Scoring**: Models assign confidence scores to entity predictions for filtering and uncertainty quantification.
* **Precision**: A performance metric for NER, the proportion of correctly identified entities among all predicted entities.
* **Recall**: A performance metric for NER, the proportion of actual entities correctly identified by the system.
* **F1-Score**: A performance metric for NER, the harmonic mean of precision and recall.

**Libraries Used and Their Purpose:**

* **`spacy`**: A leading open-source library for advanced Natural Language Processing, providing pre-trained models for efficient entity extraction and capabilities for custom entity recognition.
* **`spacy.load`**: Used to load pre-trained spaCy language models (e.g., "en_core_web_sm").
* **`spacy.explain`**: A utility within spaCy to get a human-readable description of an entity label.
* **`collections.Counter`**: Used to count hashable objects, specifically for counting entity types.
* **`collections.defaultdict`**: A subclass of `dict` that calls a factory function to supply missing values, useful for grouping entities by type.
* **`pandas`**: A library for data manipulation and analysis, used for creating DataFrames for comparison of entity counts across different document types.
* **`matplotlib.pyplot`**: A plotting library used for visualizing entity distribution (bar charts, pie charts).
* **`seaborn`**: A data visualization library based on matplotlib, used for creating statistical graphics in visualizations.
* **`spacy.training.Example`**: Used to create training examples in the spaCy format for custom NER model training.
* **`spacy.util.minibatch`**: A utility for creating mini-batches during model training.
* **`random`**: Python's built-in module for generating random numbers, used for shuffling training examples.
* **`subprocess`**: Python's built-in module for running new applications or commands, used to download spaCy models if not already present.
* **`sys`**: Python's built-in module for system-specific parameters and functions, used with `subprocess`.

---

## 7.4 Question Answering (QA)

**Terms, Acronyms, and Definitions:**

* **QA Systems (Question Answering Systems)**: Systems that automatically respond to user inquiries by analyzing context or content, leveraging Natural Language Processing (NLP) models.
* **NLP (Natural Language Processing)**: A field of AI that involves models used by QA systems to interpret questions and context.
* **BERT (Bidirectional Encoder Representations from Transformers)**: A transformer-based model developed by Google AI that revolutionized NLP by introducing bidirectional context encoding.
* **Transformer Architecture**: The neural network architecture utilized by BERT and T5, featuring stacked self-attention layers and feedforward neural networks, excelling at capturing long-range dependencies.
* **Bidirectional Encoding**: BERT's approach to processing text in both directions (left-to-right and right-to-left) at all levels, capturing rich contextual relationships.
* **Pre-training**: The first stage of BERT's training process on massive text corpora using unsupervised objectives like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).
* **Fine-tuning**: The second stage of BERT's training process where pre-trained parameters are adapted on downstream tasks (e.g., question answering) with labeled data.
* **MLM (Masked Language Modeling)**: An unsupervised pre-training objective for BERT where the model learns to predict masked words in a sentence.
* **NSP (Next Sentence Prediction)**: An unsupervised pre-training objective for BERT where the model learns to determine whether two sentences are sequential in the original corpus.
* **Contextual Embeddings**: Word embeddings generated by BERT that reflect the contextual meaning of words within sentences, differentiating words based on surrounding context.
* **Attention Mechanism**: A multi-head self-attention mechanism in BERT that enables the model to focus on different parts of the input sequence, improving information capture.
* **SQuAD (Stanford Question Answering Dataset)**: A benchmark dataset on which BERT-based QA systems have achieved outstanding performance.
* **T5 (Text-To-Text Transfer Transformer)**: A transformer-based model developed by Google AI that frames all NLP tasks as text-to-text problems.
* **Text-to-Text Approach**: T5's fundamental innovation where both inputs and outputs are text strings, providing a unified and flexible interface for all NLP tasks.
* **Encoder-Decoder Structure**: The architecture employed by T5, where an encoder processes input text and a decoder generates output text, making it suitable for sequence-to-sequence tasks.
* **Model Compression**: Techniques to compress T5 models to create smaller, more efficient variants for resource-constrained environments.
* **Domain Adaptation**: Methods for adapting T5's performance to specialized tasks and domains.
* **Multimodal Learning**: Extending T5's capabilities to handle inputs and outputs from multiple modalities, such as text, images, audio, and video.

**Libraries Used and Their Purpose:**

* **`transformers` (Hugging Face Transformers library)**: A widely used library that provides convenient access to pre-trained transformer models like BERT and T5, along with tools for their use in various NLP tasks (e.g., `pipeline`, `T5ForConditionalGeneration`, `T5Tokenizer`).
* **`torch` (PyTorch)**: The underlying deep learning framework on which some Hugging Face models are built, necessary for running the transformer models.
* **`re` (Regular Expressions)**: Python's built-in module for working with regular expressions, used for cleaning and preprocessing context text (e.g., removing excessive whitespace).
* **`typing`**: Python's built-in module for type hints, used for clearer function signatures (e.g., `List`, `Dict`, `Tuple`).
