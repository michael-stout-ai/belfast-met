{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing Dataset Regression Analysis\n",
    "\n",
    "## ⚠️ Important Note\n",
    "**The Boston Housing dataset has been deprecated in scikit-learn due to ethical concerns regarding its data collection methodology and potential for reinforcing harmful biases. This notebook is provided for educational purposes to understand regression techniques, but should not be used as a reference for real-world housing price prediction without addressing these ethical considerations.**\n",
    "\n",
    "## Overview\n",
    "The Boston Housing dataset contains information about housing in the area of Boston Mass. It was originally published in 1978 and contains 506 samples with 13 features each. The task is to predict the median value of owner-occupied homes (in thousands of dollars).\n",
    "\n",
    "## Dataset Details\n",
    "- **Samples**: 506 housing records\n",
    "- **Features**: 13 numerical features\n",
    "- **Target**: Median home value (continuous variable)\n",
    "- **Task**: Regression (predicting continuous values)\n",
    "- **Application**: Real estate valuation, economic analysis\n",
    "\n",
    "## Features Description\n",
    "1. **CRIM**: Per capita crime rate by town\n",
    "2. **ZN**: Proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. **INDUS**: Proportion of non-retail business acres per town\n",
    "4. **CHAS**: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "5. **NOX**: Nitric oxides concentration (parts per 10 million)\n",
    "6. **RM**: Average number of rooms per dwelling\n",
    "7. **AGE**: Proportion of owner-occupied units built prior to 1940\n",
    "8. **DIS**: Weighted distances to five Boston employment centres\n",
    "9. **RAD**: Index of accessibility to radial highways\n",
    "10. **TAX**: Full-value property-tax rate per $10,000\n",
    "11. **PTRATIO**: Pupil-teacher ratio by town\n",
    "12. **B**: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13. **LSTAT**: % lower status of the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "Import libraries for regression analysis, visualization, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, GridSearchCV, \n",
    "    learning_curve, validation_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    explained_variance_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"⚠️  Note: This dataset is deprecated due to ethical concerns.\")\n",
    "print(\"This analysis is for educational purposes only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Dataset\n",
    "Load the Boston Housing dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing dataset with data_home parameter to handle deprecation\n",
    "try:\n",
    "    boston = load_boston()\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"The Boston Housing dataset has been removed from scikit-learn.\")\n",
    "    print(\"Creating synthetic data for demonstration purposes...\")\n",
    "    \n",
    "    # Create synthetic data that mimics the Boston Housing dataset structure\n",
    "    np.random.seed(42)\n",
    "    n_samples = 506\n",
    "    \n",
    "    # Create synthetic features\n",
    "    data = np.column_stack([\n",
    "        np.random.exponential(3, n_samples),  # CRIM\n",
    "        np.random.exponential(12, n_samples),  # ZN\n",
    "        np.random.beta(2, 5, n_samples) * 25,  # INDUS\n",
    "        np.random.binomial(1, 0.07, n_samples),  # CHAS\n",
    "        np.random.normal(0.55, 0.1, n_samples),  # NOX\n",
    "        np.random.normal(6.3, 0.7, n_samples),  # RM\n",
    "        np.random.beta(5, 2, n_samples) * 100,  # AGE\n",
    "        np.random.exponential(4, n_samples),  # DIS\n",
    "        np.random.choice(range(1, 25), n_samples),  # RAD\n",
    "        np.random.normal(400, 150, n_samples),  # TAX\n",
    "        np.random.normal(18, 2, n_samples),  # PTRATIO\n",
    "        np.random.beta(10, 1, n_samples) * 400,  # B\n",
    "        np.random.beta(2, 8, n_samples) * 35  # LSTAT\n",
    "    ])\n",
    "    \n",
    "    # Create synthetic target (house prices) based on features\n",
    "    target = (50 - data[:, 0] * 0.1 + data[:, 5] * 8 - data[:, 12] * 0.5 + \n",
    "              np.random.normal(0, 5, n_samples))\n",
    "    target = np.clip(target, 5, 50)  # Clip to reasonable range\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', \n",
    "                    'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "    \n",
    "    # Create a boston-like object\n",
    "    class SyntheticBoston:\n",
    "        def __init__(self, data, target, feature_names):\n",
    "            self.data = data\n",
    "            self.target = target\n",
    "            self.feature_names = feature_names\n",
    "    \n",
    "    boston = SyntheticBoston(data, target, feature_names)\n",
    "    print(\"Synthetic dataset created for demonstration.\")\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['target'] = boston.target\n",
    "df['price'] = boston.target  # More intuitive name\n",
    "\n",
    "print(\"\\nDataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nTarget Variable Statistics:\")\n",
    "print(f\"Mean house price: ${df['price'].mean():.2f}k\")\n",
    "print(f\"Median house price: ${df['price'].median():.2f}k\")\n",
    "print(f\"Price range: ${df['price'].min():.2f}k - ${df['price'].max():.2f}k\")\n",
    "print(f\"Standard deviation: ${df['price'].std():.2f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Comprehensive Data Analysis\n",
    "Perform detailed statistical analysis and identify patterns in the housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found.\")\n",
    "else:\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "# Outlier detection using IQR method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "print(\"\\nOutlier Analysis:\")\n",
    "outlier_summary = []\n",
    "for column in boston.feature_names + ['price']:\n",
    "    n_outliers, lower, upper = detect_outliers_iqr(df, column)\n",
    "    outlier_summary.append({\n",
    "        'Feature': column,\n",
    "        'Outliers': n_outliers,\n",
    "        'Percentage': f\"{n_outliers/len(df)*100:.1f}%\",\n",
    "        'Lower_Bound': f\"{lower:.2f}\",\n",
    "        'Upper_Bound': f\"{upper:.2f}\"\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df.to_string(index=False))\n",
    "\n",
    "# Feature scaling analysis\n",
    "print(\"\\nFeature Scaling Analysis:\")\n",
    "scaling_analysis = pd.DataFrame({\n",
    "    'Feature': boston.feature_names,\n",
    "    'Mean': df[boston.feature_names].mean(),\n",
    "    'Std': df[boston.feature_names].std(),\n",
    "    'Min': df[boston.feature_names].min(),\n",
    "    'Max': df[boston.feature_names].max(),\n",
    "    'Range': df[boston.feature_names].max() - df[boston.feature_names].min()\n",
    "})\n",
    "\n",
    "print(\"Features with largest ranges (indicating need for scaling):\")\n",
    "print(scaling_analysis.nlargest(5, 'Range')[['Feature', 'Mean', 'Std', 'Range']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Comprehensive Data Visualization\n",
    "Create detailed visualizations to understand relationships and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution analysis\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Price distribution\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.hist(df['price'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(df['price'].mean(), color='red', linestyle='--', label=f'Mean: ${df[\"price\"].mean():.1f}k')\n",
    "plt.axvline(df['price'].median(), color='green', linestyle='--', label=f'Median: ${df[\"price\"].median():.1f}k')\n",
    "plt.xlabel('House Price ($k)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of House Prices')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot for normality check\n",
    "plt.subplot(2, 4, 2)\n",
    "stats.probplot(df['price'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot: Price vs Normal Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Log-transformed price distribution\n",
    "plt.subplot(2, 4, 3)\n",
    "log_price = np.log(df['price'])\n",
    "plt.hist(log_price, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "plt.xlabel('Log(House Price)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Log-Transformed Price Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot of prices\n",
    "plt.subplot(2, 4, 4)\n",
    "plt.boxplot(df['price'], vert=True)\n",
    "plt.ylabel('House Price ($k)')\n",
    "plt.title('Price Distribution (Box Plot)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature distributions (first 4 features)\n",
    "for i, feature in enumerate(boston.feature_names[:4]):\n",
    "    plt.subplot(2, 4, 5 + i)\n",
    "    plt.hist(df[feature], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Correlation matrix\n",
    "plt.subplot(2, 2, 1)\n",
    "correlation_matrix = df[boston.feature_names + ['price']].corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "\n",
    "# Correlation with target variable\n",
    "plt.subplot(2, 2, 2)\n",
    "target_corr = correlation_matrix['price'].drop('price').sort_values(key=abs, ascending=False)\n",
    "colors = ['red' if x < 0 else 'blue' for x in target_corr.values]\n",
    "bars = plt.barh(range(len(target_corr)), target_corr.values, color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(target_corr)), target_corr.index)\n",
    "plt.xlabel('Correlation with House Price')\n",
    "plt.title('Feature Correlation with Target Variable')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation values on bars\n",
    "for i, (bar, corr) in enumerate(zip(bars, target_corr.values)):\n",
    "    plt.text(corr + (0.02 if corr > 0 else -0.02), i, f'{corr:.3f}', \n",
    "             va='center', ha='left' if corr > 0 else 'right', fontsize=9)\n",
    "\n",
    "# Scatter plots of top correlated features\n",
    "top_features = target_corr.head(3).index\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(2, 2, 3 + i)\n",
    "    plt.scatter(df[feature], df['price'], alpha=0.6, s=30)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df[feature], df['price'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(df[feature], p(df[feature]), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('House Price ($k)')\n",
    "    plt.title(f'{feature} vs Price (r={target_corr[feature]:.3f})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation insights\n",
    "print(\"Correlation Analysis Insights:\")\n",
    "print(f\"Strongest positive correlation: {target_corr.idxmax()} ({target_corr.max():.3f})\")\n",
    "print(f\"Strongest negative correlation: {target_corr.idxmin()} ({target_corr.min():.3f})\")\n",
    "\n",
    "strong_corr_features = target_corr[abs(target_corr) > 0.5]\n",
    "print(f\"\\nFeatures with strong correlation (|r| > 0.5):\")\n",
    "for feature, corr in strong_corr_features.items():\n",
    "    print(f\"  {feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature relationships and multicollinearity analysis\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Pairwise relationships for selected features\n",
    "important_features = ['LSTAT', 'RM', 'PTRATIO', 'INDUS', 'price']\n",
    "subset_df = df[important_features]\n",
    "\n",
    "# Create pairplot\n",
    "sns.pairplot(subset_df, diag_kind='hist', plot_kws={'alpha': 0.6, 's': 30})\n",
    "plt.suptitle('Pairwise Relationships - Key Features', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Analyze multicollinearity\n",
    "high_corr_pairs = []\n",
    "feature_corr = correlation_matrix.drop('price', axis=0).drop('price', axis=1)\n",
    "for i in range(len(feature_corr.columns)):\n",
    "    for j in range(i+1, len(feature_corr.columns)):\n",
    "        corr_val = feature_corr.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr_pairs.append((feature_corr.columns[i], feature_corr.columns[j], corr_val))\n",
    "\n",
    "print(\"\\nMulticollinearity Analysis:\")\n",
    "print(\"Feature pairs with high correlation (|r| > 0.7):\")\n",
    "if high_corr_pairs:\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  {feat1} - {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"  No feature pairs with correlation > 0.7\")\n",
    "\n",
    "# Feature importance using variance\n",
    "feature_variance = df[boston.feature_names].var().sort_values(ascending=False)\n",
    "print(f\"\\nFeatures by variance (indication of information content):\")\n",
    "for feature, variance in feature_variance.head(8).items():\n",
    "    print(f\"  {feature}: {variance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Engineering and Preprocessing\n",
    "Prepare features for regression modeling with scaling, selection, and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Feature dimensions: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(f\"Training set - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"Test set - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeature scaling completed:\")\n",
    "print(f\"Original range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\n",
    "print(f\"Scaled range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")\n",
    "\n",
    "# Feature selection using different methods\n",
    "# 1. Univariate feature selection\n",
    "selector_f = SelectKBest(f_regression, k=8)\n",
    "X_train_selected = selector_f.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector_f.transform(X_test_scaled)\n",
    "\n",
    "selected_features = np.array(boston.feature_names)[selector_f.get_support()]\n",
    "feature_scores = selector_f.scores_[selector_f.get_support()]\n",
    "\n",
    "print(f\"\\nTop 8 features by F-test:\")\n",
    "for feature, score in zip(selected_features, feature_scores):\n",
    "    print(f\"  {feature}: {score:.2f}\")\n",
    "\n",
    "# 2. Recursive feature elimination\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_for_rfe = LinearRegression()\n",
    "rfe_selector = RFE(lr_for_rfe, n_features_to_select=8)\n",
    "X_train_rfe = rfe_selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_rfe = rfe_selector.transform(X_test_scaled)\n",
    "\n",
    "rfe_features = np.array(boston.feature_names)[rfe_selector.get_support()]\n",
    "print(f\"\\nTop 8 features by RFE:\")\n",
    "for i, feature in enumerate(rfe_features):\n",
    "    print(f\"  {i+1}. {feature}\")\n",
    "\n",
    "# 3. Polynomial features (for interaction effects)\n",
    "poly_features = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train_selected)  # Use selected features to avoid explosion\n",
    "X_test_poly = poly_features.transform(X_test_selected)\n",
    "\n",
    "print(f\"\\nPolynomial features:\")\n",
    "print(f\"Original features: {X_train_selected.shape[1]}\")\n",
    "print(f\"With interactions: {X_train_poly.shape[1]}\")\n",
    "\n",
    "# Compare feature selection methods\n",
    "common_features = set(selected_features) & set(rfe_features)\n",
    "print(f\"\\nCommon features between F-test and RFE: {len(common_features)}\")\n",
    "for feature in common_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# Target transformation analysis\n",
    "# Check if log transformation improves normality\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(y_train, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('Original Target Distribution')\n",
    "plt.xlabel('House Price ($k)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "log_y_train = np.log(y_train)\n",
    "plt.hist(log_y_train, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.title('Log-Transformed Target')\n",
    "plt.xlabel('Log(House Price)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sqrt_y_train = np.sqrt(y_train)\n",
    "plt.hist(sqrt_y_train, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
    "plt.title('Square Root Transformed Target')\n",
    "plt.xlabel('√(House Price)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate normality statistics\n",
    "from scipy.stats import shapiro, normaltest\n",
    "\n",
    "def test_normality(data, name):\n",
    "    shapiro_stat, shapiro_p = shapiro(data)\n",
    "    dagostino_stat, dagostino_p = normaltest(data)\n",
    "    return {\n",
    "        'transformation': name,\n",
    "        'shapiro_p': shapiro_p,\n",
    "        'dagostino_p': dagostino_p,\n",
    "        'skewness': stats.skew(data),\n",
    "        'kurtosis': stats.kurtosis(data)\n",
    "    }\n",
    "\n",
    "normality_tests = [\n",
    "    test_normality(y_train, 'Original'),\n",
    "    test_normality(log_y_train, 'Log'),\n",
    "    test_normality(sqrt_y_train, 'Square Root')\n",
    "]\n",
    "\n",
    "normality_df = pd.DataFrame(normality_tests)\n",
    "print(\"\\nNormality Test Results:\")\n",
    "print(normality_df.round(4))\n",
    "print(\"\\nNote: Higher p-values indicate more normal distribution\")\n",
    "print(\"Skewness close to 0 and kurtosis close to 0 indicate normality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Comprehensive Regression Model Training\n",
    "Train multiple regression models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize regression models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso Regression': Lasso(alpha=1.0, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Extra Trees': ExtraTreesRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR (RBF)': SVR(kernel='rbf'),\n",
    "    'SVR (Linear)': SVR(kernel='linear'),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42, max_depth=10),\n",
    "    'Neural Network': MLPRegressor(random_state=42, max_iter=1000, hidden_layer_sizes=(100, 50))\n",
    "}\n",
    "\n",
    "# Different data configurations\n",
    "data_configs = {\n",
    "    'Original': (X_train, X_test, y_train, y_test),\n",
    "    'Scaled': (X_train_scaled, X_test_scaled, y_train, y_test),\n",
    "    'Selected (F-test)': (X_train_selected, X_test_selected, y_train, y_test),\n",
    "    'Selected (RFE)': (X_train_rfe, X_test_rfe, y_train, y_test),\n",
    "    'Polynomial': (X_train_poly, X_test_poly, y_train, y_test),\n",
    "    'Log Target': (X_train_scaled, X_test_scaled, log_y_train, np.log(y_test))\n",
    "}\n",
    "\n",
    "# Regression metrics calculation\n",
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'mse': mean_squared_error(y_true, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'r2': r2_score(y_true, y_pred),\n",
    "        'explained_variance': explained_variance_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "best_model_info = {'r2': -np.inf, 'config': None, 'model': None, 'name': None}\n",
    "\n",
    "print(\"Training and evaluating regression models...\\n\")\n",
    "\n",
    "for config_name, (X_tr, X_te, y_tr, y_te) in data_configs.items():\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Configuration: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n--- {model_name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Train the model\n",
    "            model.fit(X_tr, y_tr)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_te)\n",
    "            \n",
    "            # For log-transformed target, convert back to original scale\n",
    "            if config_name == 'Log Target':\n",
    "                y_pred_original = np.exp(y_pred)\n",
    "                y_te_original = np.exp(y_te)\n",
    "                metrics = calculate_regression_metrics(y_test, y_pred_original)\n",
    "            else:\n",
    "                metrics = calculate_regression_metrics(y_te, y_pred)\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_tr, y_tr, cv=5, \n",
    "                                      scoring='r2')\n",
    "            \n",
    "            # Store results\n",
    "            config_results[model_name] = {\n",
    "                'model': model,\n",
    "                'predictions': y_pred,\n",
    "                'metrics': metrics,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std()\n",
    "            }\n",
    "            \n",
    "            # Track best model by R²\n",
    "            if metrics['r2'] > best_model_info['r2']:\n",
    "                best_model_info = {\n",
    "                    'r2': metrics['r2'],\n",
    "                    'config': config_name,\n",
    "                    'model': model,\n",
    "                    'name': model_name,\n",
    "                    'results': config_results[model_name]\n",
    "                }\n",
    "            \n",
    "            # Print key metrics\n",
    "            print(f\"R² Score: {metrics['r2']:.4f}\")\n",
    "            print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "            print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "            print(f\"CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name}: {str(e)}\")\n",
    "    \n",
    "    results[config_name] = config_results\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL OVERALL\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Configuration: {best_model_info['config']}\")\n",
    "print(f\"Model: {best_model_info['name']}\")\n",
    "print(f\"R² Score: {best_model_info['r2']:.4f}\")\n",
    "print(f\"RMSE: {best_model_info['results']['metrics']['rmse']:.4f}\")\n",
    "print(f\"CV R²: {best_model_info['results']['cv_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Performance Analysis and Visualization\n",
    "Comprehensive analysis of model performance with detailed visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results DataFrame\n",
    "results_data = []\n",
    "for config_name, config_results in results.items():\n",
    "    for model_name, model_results in config_results.items():\n",
    "        metrics = model_results['metrics']\n",
    "        results_data.append({\n",
    "            'Configuration': config_name,\n",
    "            'Model': model_name,\n",
    "            'R2': metrics['r2'],\n",
    "            'RMSE': metrics['rmse'],\n",
    "            'MAE': metrics['mae'],\n",
    "            'MSE': metrics['mse'],\n",
    "            'Explained_Variance': metrics['explained_variance'],\n",
    "            'CV_Mean': model_results['cv_mean'],\n",
    "            'CV_Std': model_results['cv_std']\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df_sorted = results_df.sort_values('R2', ascending=False)\n",
    "\n",
    "print(\"Top 15 Models by R² Score:\")\n",
    "print(results_df_sorted.head(15)[['Model', 'Configuration', 'R2', 'RMSE', 'MAE', 'CV_Mean']].to_string(index=False))\n",
    "\n",
    "# Model performance visualization\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# R² scores heatmap\n",
    "plt.subplot(2, 3, 1)\n",
    "pivot_r2 = results_df.pivot_table(\n",
    "    values='R2', index='Model', columns='Configuration', aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(pivot_r2, annot=True, cmap='RdYlGn', center=0.5, fmt='.3f')\n",
    "plt.title('R² Scores by Model and Configuration')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# RMSE comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "pivot_rmse = results_df.pivot_table(\n",
    "    values='RMSE', index='Model', columns='Configuration', aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(pivot_rmse, annot=True, cmap='RdYlGn_r', fmt='.2f')\n",
    "plt.title('RMSE by Model and Configuration')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Top 10 models bar chart\n",
    "plt.subplot(2, 3, 3)\n",
    "top_10 = results_df_sorted.head(10)\n",
    "model_labels = [f\"{row['Model']}\\n({row['Configuration']})\" for _, row in top_10.iterrows()]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_10)))\n",
    "\n",
    "bars = plt.barh(range(len(top_10)), top_10['R2'], color=colors)\n",
    "plt.yticks(range(len(top_10)), [label.replace(' (', '\\n(') for label in model_labels])\n",
    "plt.xlabel('R² Score')\n",
    "plt.title('Top 10 Models by R² Score')\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "# Add R² values on bars\n",
    "for i, (bar, r2) in enumerate(zip(bars, top_10['R2'])):\n",
    "    plt.text(r2 + 0.01, i, f'{r2:.3f}', va='center', ha='left', fontsize=8)\n",
    "\n",
    "# Best model predictions vs actual\n",
    "plt.subplot(2, 3, 4)\n",
    "best_config = best_model_info['config']\n",
    "best_name = best_model_info['name']\n",
    "best_predictions = best_model_info['results']['predictions']\n",
    "\n",
    "# Get the correct y_test values for comparison\n",
    "if best_config == 'Log Target':\n",
    "    y_test_compare = y_test\n",
    "    y_pred_compare = np.exp(best_predictions)\n",
    "else:\n",
    "    y_test_compare = y_test\n",
    "    y_pred_compare = best_predictions\n",
    "\n",
    "plt.scatter(y_test_compare, y_pred_compare, alpha=0.6, s=30)\n",
    "plt.plot([y_test_compare.min(), y_test_compare.max()], \n",
    "         [y_test_compare.min(), y_test_compare.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual House Price ($k)')\n",
    "plt.ylabel('Predicted House Price ($k)')\n",
    "plt.title(f'Best Model: {best_name}\\nR² = {best_model_info[\"r2\"]:.4f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual analysis\n",
    "plt.subplot(2, 3, 5)\n",
    "residuals = y_test_compare - y_pred_compare\n",
    "plt.scatter(y_pred_compare, residuals, alpha=0.6, s=30)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted House Price ($k)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Best Model)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Model complexity vs performance\n",
    "plt.subplot(2, 3, 6)\n",
    "# Define model complexity (subjective ranking)\n",
    "complexity_map = {\n",
    "    'Linear Regression': 1,\n",
    "    'Ridge Regression': 2,\n",
    "    'Lasso Regression': 2,\n",
    "    'ElasticNet': 3,\n",
    "    'K-Nearest Neighbors': 3,\n",
    "    'Decision Tree': 4,\n",
    "    'SVR (Linear)': 4,\n",
    "    'SVR (RBF)': 5,\n",
    "    'Random Forest': 6,\n",
    "    'Extra Trees': 6,\n",
    "    'Gradient Boosting': 7,\n",
    "    'Neural Network': 8\n",
    "}\n",
    "\n",
    "# Get best result for each model type\n",
    "best_per_model = results_df.groupby('Model')['R2'].max().reset_index()\n",
    "best_per_model['Complexity'] = best_per_model['Model'].map(complexity_map)\n",
    "\n",
    "plt.scatter(best_per_model['Complexity'], best_per_model['R2'], \n",
    "           s=100, alpha=0.7, c=best_per_model['R2'], cmap='viridis')\n",
    "plt.xlabel('Model Complexity (1=Simple, 8=Complex)')\n",
    "plt.ylabel('Best R² Score')\n",
    "plt.title('Model Complexity vs Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add model names as annotations\n",
    "for _, row in best_per_model.iterrows():\n",
    "    plt.annotate(row['Model'].replace(' ', '\\n'), \n",
    "                (row['Complexity'], row['R2']), \n",
    "                xytext=(5, 5), textcoords='offset points', \n",
    "                fontsize=8, alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis of results\n",
    "print(f\"\\nModel Performance Statistics:\")\n",
    "print(f\"Best R² Score: {results_df['R2'].max():.4f}\")\n",
    "print(f\"Worst R² Score: {results_df['R2'].min():.4f}\")\n",
    "print(f\"Mean R² Score: {results_df['R2'].mean():.4f}\")\n",
    "print(f\"Std R² Score: {results_df['R2'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nBest RMSE: {results_df['RMSE'].min():.4f}\")\n",
    "print(f\"Worst RMSE: {results_df['RMSE'].max():.4f}\")\n",
    "print(f\"Mean RMSE: {results_df['RMSE'].mean():.4f}\")\n",
    "\n",
    "# Configuration analysis\n",
    "config_performance = results_df.groupby('Configuration').agg({\n",
    "    'R2': ['mean', 'std', 'max'],\n",
    "    'RMSE': ['mean', 'std', 'min']\n",
    "}).round(4)\n",
    "\n",
    "print(f\"\\nConfiguration Performance Summary:\")\n",
    "print(config_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Feature Importance and Model Interpretation\n",
    "Analyze feature importance and model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Linear regression coefficients (from scaled data)\n",
    "plt.subplot(2, 3, 1)\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_coefs = lr_model.coef_\n",
    "\n",
    "# Sort by absolute coefficient value\n",
    "coef_abs = np.abs(lr_coefs)\n",
    "sorted_idx = np.argsort(coef_abs)[::-1]\n",
    "\n",
    "colors = ['red' if c < 0 else 'blue' for c in lr_coefs[sorted_idx]]\n",
    "bars = plt.barh(range(len(lr_coefs)), lr_coefs[sorted_idx], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(lr_coefs)), [boston.feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Linear Regression Coefficients\\n(Standardized Features)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Random Forest feature importance\n",
    "plt.subplot(2, 3, 2)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_importances = rf_model.feature_importances_\n",
    "\n",
    "rf_sorted_idx = np.argsort(rf_importances)[::-1]\n",
    "plt.barh(range(len(rf_importances)), rf_importances[rf_sorted_idx], \n",
    "         color='green', alpha=0.7)\n",
    "plt.yticks(range(len(rf_importances)), [boston.feature_names[i] for i in rf_sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Correlation-based importance\n",
    "plt.subplot(2, 3, 3)\n",
    "target_corr = np.abs(correlation_matrix['price'].drop('price'))\n",
    "corr_sorted_idx = target_corr.argsort()[::-1]\n",
    "\n",
    "plt.barh(range(len(target_corr)), target_corr.iloc[corr_sorted_idx], \n",
    "         color='orange', alpha=0.7)\n",
    "plt.yticks(range(len(target_corr)), target_corr.index[corr_sorted_idx])\n",
    "plt.xlabel('Absolute Correlation with Price')\n",
    "plt.title('Correlation-Based Importance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature importance comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "# Normalize all importance measures for comparison\n",
    "lr_coefs_norm = np.abs(lr_coefs) / np.max(np.abs(lr_coefs))\n",
    "rf_importances_norm = rf_importances / np.max(rf_importances)\n",
    "corr_norm = target_corr.values / np.max(target_corr.values)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': boston.feature_names,\n",
    "    'Linear_Regression': lr_coefs_norm,\n",
    "    'Random_Forest': rf_importances_norm,\n",
    "    'Correlation': corr_norm\n",
    "})\n",
    "\n",
    "# Calculate average importance\n",
    "importance_df['Average'] = importance_df[['Linear_Regression', 'Random_Forest', 'Correlation']].mean(axis=1)\n",
    "importance_df_sorted = importance_df.sort_values('Average', ascending=False)\n",
    "\n",
    "x = np.arange(len(importance_df_sorted))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, importance_df_sorted['Linear_Regression'], width, \n",
    "        label='Linear Regression', alpha=0.7)\n",
    "plt.bar(x, importance_df_sorted['Random_Forest'], width, \n",
    "        label='Random Forest', alpha=0.7)\n",
    "plt.bar(x + width, importance_df_sorted['Correlation'], width, \n",
    "        label='Correlation', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Normalized Importance')\n",
    "plt.title('Feature Importance Comparison')\n",
    "plt.xticks(x, importance_df_sorted['Feature'], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Partial dependence plots for top 2 features\n",
    "top_2_features = importance_df_sorted.head(2)['Feature'].values\n",
    "feature_indices = [list(boston.feature_names).index(feat) for feat in top_2_features]\n",
    "\n",
    "for i, (feature_name, feature_idx) in enumerate(zip(top_2_features, feature_indices)):\n",
    "    plt.subplot(2, 3, 5 + i)\n",
    "    \n",
    "    # Create range of values for the feature\n",
    "    feature_range = np.linspace(X_train[:, feature_idx].min(), \n",
    "                               X_train[:, feature_idx].max(), 50)\n",
    "    \n",
    "    # Use the best model for partial dependence\n",
    "    predictions = []\n",
    "    X_temp = X_train_scaled.mean(axis=0).reshape(1, -1).repeat(len(feature_range), axis=0)\n",
    "    \n",
    "    if best_model_info['config'] == 'Scaled':\n",
    "        # Scale the feature values\n",
    "        feature_scaled = scaler.transform(\n",
    "            np.column_stack([np.zeros((len(feature_range), X_train.shape[1])) \n",
    "                           for _ in range(1)])\n",
    "        )\n",
    "        for j, val in enumerate(feature_range):\n",
    "            X_temp[j, feature_idx] = (val - X_train[:, feature_idx].mean()) / X_train[:, feature_idx].std()\n",
    "    else:\n",
    "        for j, val in enumerate(feature_range):\n",
    "            X_temp[j, feature_idx] = val\n",
    "    \n",
    "    pred_vals = best_model_info['model'].predict(X_temp)\n",
    "    \n",
    "    plt.plot(feature_range, pred_vals, linewidth=2, color='purple')\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('Predicted Price')\n",
    "    plt.title(f'Partial Dependence: {feature_name}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance rankings\n",
    "print(\"Feature Importance Rankings:\")\n",
    "print(\"\\nTop 5 features by method:\")\n",
    "\n",
    "print(\"\\nLinear Regression (absolute coefficients):\")\n",
    "for i, idx in enumerate(sorted_idx[:5]):\n",
    "    print(f\"  {i+1}. {boston.feature_names[idx]}: {lr_coefs[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "for i, idx in enumerate(rf_sorted_idx[:5]):\n",
    "    print(f\"  {i+1}. {boston.feature_names[idx]}: {rf_importances[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nCorrelation:\")\n",
    "for i, feature in enumerate(target_corr.index[corr_sorted_idx[:5]]):\n",
    "    print(f\"  {i+1}. {feature}: {target_corr[feature]:.4f}\")\n",
    "\n",
    "print(\"\\nConsensus ranking (average of all methods):\")\n",
    "for i, _, row in enumerate(importance_df_sorted.head(8).itertuples()):\n",
    "    print(f\"  {i+1}. {row.Feature}: {row.Average:.4f}\")\n",
    "\n",
    "# Model interpretability analysis\n",
    "print(f\"\\nModel Interpretability Analysis:\")\n",
    "print(f\"\\nBest Model: {best_model_info['name']}\")\n",
    "if 'Linear' in best_model_info['name'] or 'Ridge' in best_model_info['name'] or 'Lasso' in best_model_info['name']:\n",
    "    print(\"- High interpretability: Coefficients directly show feature impact\")\n",
    "    print(\"- Each unit increase in feature leads to coefficient * unit change in price\")\n",
    "elif 'Tree' in best_model_info['name'] or 'Forest' in best_model_info['name']:\n",
    "    print(\"- Medium interpretability: Feature importance shows relative influence\")\n",
    "    print(\"- Decision paths can be traced for individual predictions\")\n",
    "elif 'SVM' in best_model_info['name'] or 'Neural' in best_model_info['name']:\n",
    "    print(\"- Low interpretability: Complex non-linear relationships\")\n",
    "    print(\"- Requires advanced techniques for interpretation\")\n",
    "else:\n",
    "    print(\"- Variable interpretability depending on model complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Hyperparameter Tuning and Model Optimization\n",
    "Optimize the best performing models with systematic hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for top performing model types\n",
    "tuning_models = {\n",
    "    'Ridge Regression': {\n",
    "        'model': Ridge(random_state=42),\n",
    "        'params': {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "        }\n",
    "    },\n",
    "    'Lasso Regression': {\n",
    "        'model': Lasso(random_state=42),\n",
    "        'params': {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "        }\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'model': ElasticNet(random_state=42),\n",
    "        'params': {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "            'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 15, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'SVR (RBF)': {\n",
    "        'model': SVR(kernel='rbf'),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "            'epsilon': [0.01, 0.1, 0.2]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use the best configuration from previous results\n",
    "best_config = best_model_info['config']\n",
    "if best_config == 'Scaled':\n",
    "    X_tune, X_test_tune = X_train_scaled, X_test_scaled\n",
    "elif best_config == 'Selected (F-test)':\n",
    "    X_tune, X_test_tune = X_train_selected, X_test_selected\n",
    "elif best_config == 'Selected (RFE)':\n",
    "    X_tune, X_test_tune = X_train_rfe, X_test_rfe\n",
    "elif best_config == 'Polynomial':\n",
    "    X_tune, X_test_tune = X_train_poly, X_test_poly\n",
    "elif best_config == 'Log Target':\n",
    "    X_tune, X_test_tune = X_train_scaled, X_test_scaled\n",
    "    y_tune, y_test_tune = log_y_train, np.log(y_test)\n",
    "else:\n",
    "    X_tune, X_test_tune = X_train, X_test\n",
    "    y_tune, y_test_tune = y_train, y_test\n",
    "\n",
    "if best_config != 'Log Target':\n",
    "    y_tune, y_test_tune = y_train, y_test\n",
    "\n",
    "tuning_results = {}\n",
    "print(f\"Hyperparameter tuning using {best_config} configuration...\\n\")\n",
    "\n",
    "for model_name, model_info in tuning_models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        model_info['model'], \n",
    "        model_info['params'], \n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_tune, y_tune)\n",
    "    \n",
    "    # Test the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = best_model.predict(X_test_tune)\n",
    "    \n",
    "    # Handle log transformation if needed\n",
    "    if best_config == 'Log Target':\n",
    "        y_pred_original = np.exp(y_pred_tuned)\n",
    "        y_test_original = y_test\n",
    "        tuned_metrics = calculate_regression_metrics(y_test_original, y_pred_original)\n",
    "    else:\n",
    "        tuned_metrics = calculate_regression_metrics(y_test_tune, y_pred_tuned)\n",
    "    \n",
    "    tuning_results[model_name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'test_metrics': tuned_metrics,\n",
    "        'model': best_model,\n",
    "        'predictions': y_pred_tuned\n",
    "    }\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV R² score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Test R² score: {tuned_metrics['r2']:.4f}\")\n",
    "    print(f\"Test RMSE: {tuned_metrics['rmse']:.4f}\\n\")\n",
    "\n",
    "# Compare with original results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "for model_name in tuning_results.keys():\n",
    "    # Find original result for this model with best config\n",
    "    if model_name in results[best_config]:\n",
    "        original_r2 = results[best_config][model_name]['metrics']['r2']\n",
    "        original_rmse = results[best_config][model_name]['metrics']['rmse']\n",
    "    else:\n",
    "        original_r2 = 0\n",
    "        original_rmse = float('inf')\n",
    "    \n",
    "    tuned_r2 = tuning_results[model_name]['test_metrics']['r2']\n",
    "    tuned_rmse = tuning_results[model_name]['test_metrics']['rmse']\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Original_R2': original_r2,\n",
    "        'Tuned_R2': tuned_r2,\n",
    "        'R2_Improvement': tuned_r2 - original_r2,\n",
    "        'Original_RMSE': original_rmse,\n",
    "        'Tuned_RMSE': tuned_rmse,\n",
    "        'RMSE_Improvement': original_rmse - tuned_rmse\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "# Find the best tuned model\n",
    "best_tuned_model = max(tuning_results.items(), key=lambda x: x[1]['test_metrics']['r2'])\n",
    "best_tuned_name, best_tuned_results = best_tuned_model\n",
    "\n",
    "print(f\"\\nBest tuned model: {best_tuned_name}\")\n",
    "print(f\"Test R² score: {best_tuned_results['test_metrics']['r2']:.4f}\")\n",
    "print(f\"Test RMSE: {best_tuned_results['test_metrics']['rmse']:.4f}\")\n",
    "print(f\"Best parameters: {best_tuned_results['best_params']}\")\n",
    "\n",
    "# Visualize tuning results\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# R² comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, comparison_df['Original_R2'], width, \n",
    "        label='Original', alpha=0.7, color='lightblue')\n",
    "plt.bar(x + width/2, comparison_df['Tuned_R2'], width, \n",
    "        label='Tuned', alpha=0.7, color='darkblue')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Original vs Tuned Model Performance (R²)')\n",
    "plt.xticks(x, comparison_df['Model'], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.bar(x - width/2, comparison_df['Original_RMSE'], width, \n",
    "        label='Original', alpha=0.7, color='lightcoral')\n",
    "plt.bar(x + width/2, comparison_df['Tuned_RMSE'], width, \n",
    "        label='Tuned', alpha=0.7, color='darkred')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Original vs Tuned Model Performance (RMSE)')\n",
    "plt.xticks(x, comparison_df['Model'], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Improvement visualization\n",
    "plt.subplot(2, 3, 3)\n",
    "colors_r2 = ['green' if imp > 0 else 'red' for imp in comparison_df['R2_Improvement']]\n",
    "bars = plt.bar(comparison_df['Model'], comparison_df['R2_Improvement'], \n",
    "               color=colors_r2, alpha=0.7)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('R² Improvement')\n",
    "plt.title('R² Improvement from Tuning')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add improvement values on bars\n",
    "for bar, imp in zip(bars, comparison_df['R2_Improvement']):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + (0.005 if height >= 0 else -0.01),\n",
    "             f'{imp:+.3f}', ha='center', va='bottom' if height >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# Best tuned model predictions vs actual\n",
    "plt.subplot(2, 3, 4)\n",
    "best_tuned_predictions = best_tuned_results['predictions']\n",
    "\n",
    "if best_config == 'Log Target':\n",
    "    y_test_compare = y_test\n",
    "    y_pred_compare = np.exp(best_tuned_predictions)\n",
    "else:\n",
    "    y_test_compare = y_test\n",
    "    y_pred_compare = best_tuned_predictions\n",
    "\n",
    "plt.scatter(y_test_compare, y_pred_compare, alpha=0.6, s=30)\n",
    "plt.plot([y_test_compare.min(), y_test_compare.max()], \n",
    "         [y_test_compare.min(), y_test_compare.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual House Price ($k)')\n",
    "plt.ylabel('Predicted House Price ($k)')\n",
    "plt.title(f'Best Tuned Model: {best_tuned_name}\\nR² = {best_tuned_results[\"test_metrics\"][\"r2\"]:.4f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual analysis for best tuned model\n",
    "plt.subplot(2, 3, 5)\n",
    "residuals = y_test_compare - y_pred_compare\n",
    "plt.scatter(y_pred_compare, residuals, alpha=0.6, s=30)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted House Price ($k)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Best Tuned Model)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning curve for best tuned model\n",
    "plt.subplot(2, 3, 6)\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "    best_tuned_results['model'], X_tune, y_tune, train_sizes=train_sizes, \n",
    "    cv=5, n_jobs=-1, random_state=42, scoring='r2'\n",
    ")\n",
    "\n",
    "plt.plot(train_sizes_abs, np.mean(train_scores, axis=1), 'o-', \n",
    "         color='blue', label='Training R²')\n",
    "plt.plot(train_sizes_abs, np.mean(val_scores, axis=1), 'o-', \n",
    "         color='red', label='Validation R²')\n",
    "plt.fill_between(train_sizes_abs, np.mean(train_scores, axis=1) - np.std(train_scores, axis=1),\n",
    "                 np.mean(train_scores, axis=1) + np.std(train_scores, axis=1), alpha=0.1, color='blue')\n",
    "plt.fill_between(train_sizes_abs, np.mean(val_scores, axis=1) - np.std(val_scores, axis=1),\n",
    "                 np.mean(val_scores, axis=1) + np.std(val_scores, axis=1), alpha=0.1, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title(f'Learning Curve\\n{best_tuned_name}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Final Model Evaluation and Real Estate Applications\n",
    "Comprehensive final evaluation and discussion of practical real estate applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the overall best model (considering both original and tuned)\n",
    "best_overall_r2 = best_model_info['r2']\n",
    "best_overall_name = best_model_info['name']\n",
    "best_overall_config = best_model_info['config']\n",
    "best_overall_model = best_model_info['model']\n",
    "best_overall_predictions = best_model_info['results']['predictions']\n",
    "best_overall_metrics = best_model_info['results']['metrics']\n",
    "\n",
    "# Check if any tuned model is better\n",
    "for model_name, tuned_results in tuning_results.items():\n",
    "    if tuned_results['test_metrics']['r2'] > best_overall_r2:\n",
    "        best_overall_r2 = tuned_results['test_metrics']['r2']\n",
    "        best_overall_name = f\"{model_name} (Tuned)\"\n",
    "        best_overall_model = tuned_results['model']\n",
    "        best_overall_predictions = tuned_results['predictions']\n",
    "        best_overall_metrics = tuned_results['test_metrics']\n",
    "\n",
    "print(f\"FINAL BEST MODEL: {best_overall_name}\")\n",
    "print(f\"Configuration: {best_overall_config}\")\n",
    "print(f\"Test R² Score: {best_overall_r2:.4f}\")\n",
    "\n",
    "# Comprehensive final evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE FINAL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFinal Model Performance Metrics:\")\n",
    "print(f\"  R² Score: {best_overall_metrics['r2']:.4f} ({best_overall_metrics['r2']*100:.1f}% variance explained)\")\n",
    "print(f\"  RMSE: ${best_overall_metrics['rmse']:.2f}k\")\n",
    "print(f\"  MAE: ${best_overall_metrics['mae']:.2f}k\")\n",
    "print(f\"  MSE: {best_overall_metrics['mse']:.2f}\")\n",
    "print(f\"  Explained Variance: {best_overall_metrics['explained_variance']:.4f}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "if best_overall_config == 'Log Target':\n",
    "    y_test_final = y_test\n",
    "    y_pred_final = np.exp(best_overall_predictions)\n",
    "else:\n",
    "    y_test_final = y_test\n",
    "    y_pred_final = best_overall_predictions\n",
    "\n",
    "# Prediction accuracy analysis\n",
    "absolute_errors = np.abs(y_test_final - y_pred_final)\n",
    "percentage_errors = (absolute_errors / y_test_final) * 100\n",
    "\n",
    "print(f\"\\nPrediction Accuracy Analysis:\")\n",
    "print(f\"  Mean Absolute Error: ${absolute_errors.mean():.2f}k\")\n",
    "print(f\"  Median Absolute Error: ${np.median(absolute_errors):.2f}k\")\n",
    "print(f\"  Mean Percentage Error: {percentage_errors.mean():.1f}%\")\n",
    "print(f\"  Median Percentage Error: {np.median(percentage_errors):.1f}%\")\n",
    "\n",
    "# Accuracy by price range\n",
    "price_ranges = [(0, 15), (15, 25), (25, 35), (35, float('inf'))]\n",
    "range_labels = ['Low ($0-15k)', 'Medium ($15-25k)', 'High ($25-35k)', 'Luxury ($35k+)']\n",
    "\n",
    "print(f\"\\nAccuracy by Price Range:\")\n",
    "for (low, high), label in zip(price_ranges, range_labels):\n",
    "    mask = (y_test_final >= low) & (y_test_final < high)\n",
    "    if mask.sum() > 0:\n",
    "        range_mae = absolute_errors[mask].mean()\n",
    "        range_mape = percentage_errors[mask].mean()\n",
    "        range_r2 = r2_score(y_test_final[mask], y_pred_final[mask])\n",
    "        print(f\"  {label}: MAE=${range_mae:.2f}k, MAPE={range_mape:.1f}%, R²={range_r2:.3f} (n={mask.sum()})\")\n",
    "\n",
    "# Model confidence analysis\n",
    "confidence_threshold_5 = np.percentile(absolute_errors, 95)  # 95% of predictions within this error\n",
    "confidence_threshold_10 = np.percentile(absolute_errors, 90)  # 90% of predictions within this error\n",
    "\n",
    "print(f\"\\nModel Confidence Analysis:\")\n",
    "print(f\"  95% of predictions within: ±${confidence_threshold_5:.2f}k\")\n",
    "print(f\"  90% of predictions within: ±${confidence_threshold_10:.2f}k\")\n",
    "print(f\"  Predictions within ±$2k: {(absolute_errors <= 2).mean()*100:.1f}%\")\n",
    "print(f\"  Predictions within ±$5k: {(absolute_errors <= 5).mean()*100:.1f}%\")\n",
    "\n",
    "# Final visualization\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Final predictions vs actual\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.scatter(y_test_final, y_pred_final, alpha=0.6, s=40, c=absolute_errors, cmap='viridis')\n",
    "plt.plot([y_test_final.min(), y_test_final.max()], \n",
    "         [y_test_final.min(), y_test_final.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "plt.colorbar(label='Absolute Error ($k)')\n",
    "plt.xlabel('Actual House Price ($k)')\n",
    "plt.ylabel('Predicted House Price ($k)')\n",
    "plt.title(f'Final Model Predictions\\n{best_overall_name}\\nR² = {best_overall_r2:.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual analysis\n",
    "plt.subplot(2, 3, 2)\n",
    "residuals_final = y_test_final - y_pred_final\n",
    "plt.scatter(y_pred_final, residuals_final, alpha=0.6, s=40)\n",
    "plt.axhline(y=0, color='r', linestyle='--', label='Zero residual')\n",
    "plt.axhline(y=residuals_final.std(), color='orange', linestyle=':', alpha=0.7, label='+1 std')\n",
    "plt.axhline(y=-residuals_final.std(), color='orange', linestyle=':', alpha=0.7, label='-1 std')\n",
    "plt.xlabel('Predicted House Price ($k)')\n",
    "plt.ylabel('Residuals ($k)')\n",
    "plt.title('Residual Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(absolute_errors, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(absolute_errors.mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: ${absolute_errors.mean():.2f}k')\n",
    "plt.axvline(np.median(absolute_errors), color='green', linestyle='--', \n",
    "           label=f'Median: ${np.median(absolute_errors):.2f}k')\n",
    "plt.xlabel('Absolute Error ($k)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Percentage error by price\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(y_test_final, percentage_errors, alpha=0.6, s=40)\n",
    "plt.axhline(y=percentage_errors.mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {percentage_errors.mean():.1f}%')\n",
    "plt.xlabel('Actual House Price ($k)')\n",
    "plt.ylabel('Percentage Error (%)')\n",
    "plt.title('Percentage Error vs House Price')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance for final model\n",
    "plt.subplot(2, 3, 5)\n",
    "if hasattr(best_overall_model, 'feature_importances_'):\n",
    "    importances = best_overall_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1][:10]\n",
    "    \n",
    "    plt.barh(range(len(indices)), importances[indices], color='lightcoral', alpha=0.7)\n",
    "    \n",
    "    # Determine feature names based on configuration\n",
    "    if best_overall_config == 'Selected (F-test)':\n",
    "        feature_names_final = selected_features\n",
    "    elif best_overall_config == 'Selected (RFE)':\n",
    "        feature_names_final = rfe_features\n",
    "    else:\n",
    "        feature_names_final = boston.feature_names\n",
    "    \n",
    "    plt.yticks(range(len(indices)), [feature_names_final[i] for i in indices])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 10 Feature Importances\\n(Final Model)')\n",
    "elif hasattr(best_overall_model, 'coef_'):\n",
    "    coefs = np.abs(best_overall_model.coef_)\n",
    "    indices = np.argsort(coefs)[::-1][:10]\n",
    "    \n",
    "    plt.barh(range(len(indices)), coefs[indices], color='lightblue', alpha=0.7)\n",
    "    \n",
    "    if best_overall_config == 'Selected (F-test)':\n",
    "        feature_names_final = selected_features\n",
    "    elif best_overall_config == 'Selected (RFE)':\n",
    "        feature_names_final = rfe_features\n",
    "    else:\n",
    "        feature_names_final = boston.feature_names\n",
    "    \n",
    "    plt.yticks(range(len(indices)), [feature_names_final[i] for i in indices])\n",
    "    plt.xlabel('Coefficient Magnitude')\n",
    "    plt.title('Top 10 Coefficient Magnitudes\\n(Final Model)')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Feature importance\\nnot available\\nfor this model type', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes, fontsize=12)\n",
    "    plt.title('Feature Importance')\n",
    "\n",
    "# Model summary\n",
    "plt.subplot(2, 3, 6)\n",
    "summary_text = f\"\"\"\n",
    "FINAL MODEL SUMMARY\n",
    "\n",
    "Model: {best_overall_name}\n",
    "Configuration: {best_overall_config}\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "• R² Score: {best_overall_r2:.4f}\n",
    "• RMSE: ${best_overall_metrics['rmse']:.2f}k\n",
    "• MAE: ${best_overall_metrics['mae']:.2f}k\n",
    "• Mean % Error: {percentage_errors.mean():.1f}%\n",
    "\n",
    "PRACTICAL ACCURACY:\n",
    "• Within ±$2k: {(absolute_errors <= 2).mean()*100:.0f}%\n",
    "• Within ±$5k: {(absolute_errors <= 5).mean()*100:.0f}%\n",
    "• 95% Confidence: ±${confidence_threshold_5:.1f}k\n",
    "\n",
    "MODEL CHARACTERISTICS:\n",
    "• Complexity: {'High' if 'Neural' in best_overall_name or 'Boosting' in best_overall_name else 'Medium' if 'Forest' in best_overall_name or 'Tree' in best_overall_name else 'Low'}\n",
    "• Interpretability: {'Low' if 'Neural' in best_overall_name or 'SVR' in best_overall_name else 'Medium' if 'Forest' in best_overall_name or 'Tree' in best_overall_name else 'High'}\n",
    "• Speed: {'Fast' if 'Linear' in best_overall_name or 'Ridge' in best_overall_name or 'Lasso' in best_overall_name else 'Medium'}\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.05, 0.95, summary_text, transform=plt.gca().transAxes, fontsize=10,\n",
    "         verticalalignment='top', fontfamily='monospace')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Real estate application analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REAL ESTATE APPLICATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModel Suitability for Real Estate Applications:\")\n",
    "\n",
    "if best_overall_r2 >= 0.80:\n",
    "    print(f\"✓ EXCELLENT for automated valuation models (AVMs)\")\n",
    "    print(f\"✓ Suitable for preliminary property assessments\")\n",
    "    print(f\"✓ Can be used for market trend analysis\")\n",
    "elif best_overall_r2 >= 0.70:\n",
    "    print(f\"✓ GOOD for supporting human appraisers\")\n",
    "    print(f\"○ Requires additional validation for high-value properties\")\n",
    "    print(f\"✓ Useful for comparative market analysis\")\n",
    "elif best_overall_r2 >= 0.60:\n",
    "    print(f\"○ FAIR for rough price estimation\")\n",
    "    print(f\"✗ Not suitable for primary valuation\")\n",
    "    print(f\"○ May be useful for market segmentation\")\n",
    "else:\n",
    "    print(f\"✗ POOR - Requires significant improvement\")\n",
    "    print(f\"✗ Not recommended for any production use\")\n",
    "\n",
    "print(f\"\\nRecommended Applications:\")\n",
    "print(f\"1. Automated Valuation Models (AVMs)\")\n",
    "print(f\"   - Quick property value estimates\")\n",
    "print(f\"   - Portfolio valuation\")\n",
    "print(f\"   - Market screening\")\n",
    "\n",
    "print(f\"\\n2. Investment Analysis\")\n",
    "print(f\"   - Property investment screening\")\n",
    "print(f\"   - ROI calculations\")\n",
    "print(f\"   - Market opportunity identification\")\n",
    "\n",
    "print(f\"\\n3. Real Estate Decision Support\")\n",
    "print(f\"   - Pricing strategy development\")\n",
    "print(f\"   - Market trend analysis\")\n",
    "print(f\"   - Comparative market analysis (CMA)\")\n",
    "\n",
    "print(f\"\\nImplementation Considerations:\")\n",
    "print(f\"• Data Requirements: {len(boston.feature_names)} property features\")\n",
    "print(f\"• Update Frequency: Monthly or quarterly model retraining recommended\")\n",
    "print(f\"• Validation: Compare predictions with recent sales for accuracy monitoring\")\n",
    "print(f\"• Human Oversight: Professional appraisal for high-value or unique properties\")\n",
    "\n",
    "print(f\"\\nLimitations and Risks:\")\n",
    "print(f\"⚠️  Dataset contains historical data that may not reflect current market conditions\")\n",
    "print(f\"⚠️  Model may not capture unique property characteristics or market anomalies\")\n",
    "print(f\"⚠️  Geographic and temporal limitations of the training data\")\n",
    "print(f\"⚠️  Ethical concerns regarding certain features in the original dataset\")\n",
    "\n",
    "print(f\"\\nETHICAL CONSIDERATIONS:\")\n",
    "print(f\"⚠️  This dataset has been deprecated due to ethical concerns\")\n",
    "print(f\"⚠️  Some features may perpetuate historical biases\")\n",
    "print(f\"⚠️  Modern implementations should focus on property characteristics only\")\n",
    "print(f\"⚠️  Regular bias auditing recommended for any production system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Real Estate Applications\n",
    "\n",
    "### ⚠️ Important Ethical Note\n",
    "**This analysis uses the deprecated Boston Housing dataset for educational purposes only. The dataset contains features that may perpetuate historical biases and should not be used as a reference for modern real estate applications without addressing these ethical concerns.**\n",
    "\n",
    "### Dataset Characteristics and Challenges\n",
    "- **Small Dataset**: 506 samples limit generalizability to broader markets\n",
    "- **Historical Data**: 1978 data may not reflect current market dynamics\n",
    "- **Feature Quality**: Mix of property-specific and neighborhood-level features\n",
    "- **Geographic Limitation**: Specific to Boston area, not generalizable globally\n",
    "\n",
    "### Model Performance Analysis\n",
    "- **Strong Predictive Power**: Top models achieve R² > 0.8, indicating good explanatory capability\n",
    "- **Reasonable Error Rates**: RMSE typically $3-5k on average home values of $22k (1978 dollars)\n",
    "- **Feature Sensitivity**: Models respond appropriately to key housing factors (rooms, location, condition)\n",
    "- **Overfitting Risk**: Small dataset size requires careful validation\n",
    "\n",
    "### Most Influential Features\n",
    "\n",
    "#### **Property Characteristics**\n",
    "- **RM (Average Rooms)**: Strong positive correlation with price\n",
    "- **AGE (Building Age)**: Older properties generally valued lower\n",
    "- **CHAS (Charles River)**: Waterfront proximity premium\n",
    "\n",
    "#### **Neighborhood Factors**\n",
    "- **LSTAT (Lower Status %)**: Strong negative correlation with housing values\n",
    "- **PTRATIO (Pupil-Teacher Ratio)**: Education quality impact on values\n",
    "- **CRIM (Crime Rate)**: Safety considerations in pricing\n",
    "\n",
    "#### **Accessibility and Infrastructure**\n",
    "- **DIS (Employment Center Distance)**: Commute convenience factor\n",
    "- **RAD (Highway Access)**: Transportation infrastructure impact\n",
    "- **TAX (Property Tax Rate)**: Tax burden effect on desirability\n",
    "\n",
    "### Modern Real Estate Applications (Considerations)\n",
    "\n",
    "#### **Automated Valuation Models (AVMs)**\n",
    "- **Primary Use**: Quick property value estimates for lending and investment\n",
    "- **Accuracy Requirements**: ±5-10% error acceptable for screening purposes\n",
    "- **Update Frequency**: Monthly retraining with recent sales data\n",
    "- **Human Validation**: Professional appraisal for high-value or unique properties\n",
    "\n",
    "#### **Investment Analysis Tools**\n",
    "- **Portfolio Screening**: Rapid assessment of multiple properties\n",
    "- **Market Opportunity Identification**: Undervalued property detection\n",
    "- **ROI Calculations**: Expected return analysis for investors\n",
    "- **Risk Assessment**: Price volatility and market stability analysis\n",
    "\n",
    "#### **Real Estate Decision Support**\n",
    "- **Pricing Strategy**: Market-based pricing recommendations for sellers\n",
    "- **Negotiation Support**: Fair value ranges for buyers and sellers\n",
    "- **Market Analysis**: Trend identification and comparative studies\n",
    "- **Development Planning**: Site selection and project viability assessment\n",
    "\n",
    "### Implementation Framework\n",
    "\n",
    "#### **Data Infrastructure**\n",
    "1. **Property Database**: Comprehensive property characteristics and transaction history\n",
    "2. **Market Data Integration**: Recent sales, listings, and economic indicators\n",
    "3. **Geographic Information**: Location-based features and neighborhood analytics\n",
    "4. **Quality Assurance**: Data validation and outlier detection systems\n",
    "\n",
    "#### **Model Architecture**\n",
    "1. **Ensemble Approach**: Combine multiple algorithms for robust predictions\n",
    "2. **Feature Engineering**: Transform raw data into predictive features\n",
    "3. **Regional Models**: Location-specific models for different markets\n",
    "4. **Temporal Updates**: Regular retraining to capture market changes\n",
    "\n",
    "#### **Validation and Monitoring**\n",
    "1. **Holdout Testing**: Reserve recent data for model validation\n",
    "2. **Prediction Tracking**: Monitor accuracy against actual sales\n",
    "3. **Market Adjustment**: Calibrate for changing market conditions\n",
    "4. **Bias Detection**: Regular auditing for fairness and discrimination\n",
    "\n",
    "### Ethical and Legal Considerations\n",
    "\n",
    "#### **Fair Housing Compliance**\n",
    "- **Protected Classes**: Ensure models don't discriminate based on race, religion, etc.\n",
    "- **Feature Selection**: Use only property-related characteristics\n",
    "- **Bias Testing**: Regular statistical testing for discriminatory outcomes\n",
    "- **Transparency**: Clear explanation of factors influencing valuations\n",
    "\n",
    "#### **Professional Standards**\n",
    "- **Appraiser Guidelines**: Complement, not replace, professional judgment\n",
    "- **Regulatory Compliance**: Adhere to local real estate and lending regulations\n",
    "- **Consumer Protection**: Clear disclosure of automated valuation limitations\n",
    "- **Professional Oversight**: Licensed professionals should validate model outputs\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "#### **Advanced Modeling Techniques**\n",
    "- **Deep Learning**: Neural networks for complex feature interactions\n",
    "- **Geospatial Models**: Incorporate detailed location and proximity data\n",
    "- **Time Series Integration**: Model temporal price trends and seasonality\n",
    "- **External Data Sources**: Economic indicators, demographics, infrastructure changes\n",
    "\n",
    "#### **Technology Integration**\n",
    "- **Computer Vision**: Automated property condition assessment from images\n",
    "- **IoT Data**: Smart home features and energy efficiency metrics\n",
    "- **Market Sentiment**: Social media and news analysis for market timing\n",
    "- **Real-time Updates**: Continuous learning from new market transactions\n",
    "\n",
    "### Limitations and Risks\n",
    "\n",
    "#### **Model Limitations**\n",
    "- **Data Dependency**: Quality limited by input data accuracy and completeness\n",
    "- **Market Uniqueness**: Difficulty capturing unique property characteristics\n",
    "- **Economic Shifts**: Models may lag during rapid market changes\n",
    "- **Geographic Scope**: Limited transferability across different markets\n",
    "\n",
    "#### **Business Risks**\n",
    "- **Over-reliance**: Automated systems shouldn't replace human expertise entirely\n",
    "- **Liability Issues**: Potential legal responsibility for inaccurate valuations\n",
    "- **Market Disruption**: Rapid technological change in real estate sector\n",
    "- **Competitive Pressure**: Need for continuous model improvement and innovation\n",
    "\n",
    "### Conclusion\n",
    "While this analysis demonstrates the technical feasibility of machine learning for housing price prediction, modern implementations must address the ethical concerns present in historical datasets. The techniques shown here provide a foundation for developing fair, accurate, and useful real estate valuation tools when applied to appropriate, unbiased datasets with proper oversight and validation procedures.\n",
    "\n",
    "**Key Recommendation**: Future real estate ML applications should focus exclusively on property characteristics and legitimate market factors while implementing robust bias detection and fairness monitoring systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
